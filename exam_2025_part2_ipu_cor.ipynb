{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1f0627a-96b6-4554-b576-7f1432aa8e14",
   "metadata": {},
   "source": [
    "### Introduction to Python and machine learning for economists (intermediate Python users) - FINAL EXAM\n",
    "\n",
    "Date: 14/11/2025\n",
    "\n",
    "**_PART 2 (10 points)_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dc72da-9eb4-45ff-80cf-a0451f97073e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Last name: \n",
    "First name:\n",
    "Student ID:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b727b542-ae2d-40e6-9f28-91c349d4c84e",
   "metadata": {},
   "source": [
    "**Data and context**\n",
    "\n",
    "* The data are based on Giacobino, Huillery, Michel and Sage (AEJ: Applied Economics 2024). \n",
    "\n",
    "* This paper studies the impact of providing scholarships for secondary education to adolescent girls in Niger on the incidence of child marriage. More specifically the authors evaluate a program of the government of Niger in which girls entering middle school (approximately 13 years old) were randomly selected to receive a scholarship covering the cost of housing, food and school supplies for three years. The authors collected baseline data on all girls admitted to middle school in 2017 in 285 villages provided that they were still enrolled in school at the time of the baseline survey in December 2017. A follow-up survey was conducted in August 2020.\n",
    "\n",
    "* You are provided with the GHMS_2024.dta database which contains data for 1,344 girls of whom 680 were randomly assigned to the scholarship (T100 = 1).\n",
    "You also have access to a dictionary for all variables in the attached xlsx file (dictionary_GHMS). The treatment variable is T100 and the two key outcomes of interest are `m_gq_dropout` and `m_gq_Married`, the school enrolment status and marriage status of girls in August 2020 respectively.\n",
    "\n",
    "* Baseline covariates are identified by the prefix ‘b_’ which is followed by ‘gq’ if the variable comes from the girls’ questionnaire (measured at the girl level) and by ‘hq’ if the variable was collected as part of the household questionnaire (information provided by an adult member of the household, generally measured at the household level).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a7e444-cd9f-4879-b1bf-732a4e491883",
   "metadata": {},
   "source": [
    "**Assignment**\n",
    "\n",
    "Using the GHMS_2024.dta data, you are asked to complete the tasks listed below. You will present your analysis in this Jupyter Notebook. You should comment each cell of code in your notebook to show that you understand what you are doing. You are allowed to write your comments in English or in French.\n",
    "\n",
    "1.\tLoad the GHMS_2024.dta dataset in a dataframe and describe it. Hint: you will need to use the read_stata function of pandas to load the data. (/1pt)\n",
    "2.  Using the sub-sample of girls assigned to the control group (C100 = 1) and the validation set approach, predict the school enrollment status of Nigerian girls approximately three years after they enter middle school (i.e: the endline enrollment status) using a **logistic** model. Use the list of baseline covariates proposed below (object `features`) as predictors. Use a seed of 3. Note: don't worry if you receive a \"RuntimeWarning:\" message at this stage. (/3pts)\n",
    "3. Obtain the predicted values of the response variable and use them to compute the accuracy rate of your model. Compare it to the accuracy rate you would obtain if you predicted the endline enrollment status by flipping a coin. What is the percentage of improvement in accuracy that you achieve?   (/2 pts)\n",
    "4. Fit a LASSO model to the same data, using the same predictors. Use 5-fold cross-validation to select the optimal lambda and a validation set to obtain the trained model. Always use a seed of 3. (/2 pts)\n",
    "5. Obtain the predicted values of the response variable for the test sample of your outer split and use them to compute the accuracy rate of your LASSO model. Compare it to the accuracy rate of your logistic model. What is the percentage of improvement in accuracy that you achieve? (/1pt). Hint: To obtain the matrix of predictor values for you test sample, you can use:\n",
    "\n",
    "`for train_idx, test_idx in outer_valid.split(X):`\n",
    "\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    Y_train, Y_test = Y.iloc[train_idx], Y.iloc[test_idx]\n",
    " \n",
    "6. Create a dataframe containing one column for the labels of predictor variables and another one with the coefficients on each predictor variable in the trained LASSO model. What is the predictor for which the coefficient has the largest absolute value? Hint : you can use DataFrame.sort_values(by = '', ascending=False) to sort your dataframe (replace DataFrame by the name of your dataframe and pass the name of the column on which you want to sort to the `by` argument). (/1pt)\n",
    "\n",
    "Note: We would normally need to use the logistic version of LASSO given that our response variable is binary. Here, we stick to standard LASSO for simplicity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9800a297-1324-4da5-a2f4-806c00e00e40",
   "metadata": {},
   "source": [
    "**List of predictors to be included in your models**\n",
    "\n",
    "features = ['b_hq_a2_wood', 'b_hq_a2_stone', 'b_hq_a2_brick', 'b_hq_a2_cement', 'b_hq_a6_Index_bw_Q2', 'b_hq_a6_Index_bw_Q3', 'b_hq_a6_Index_bw_Q4', 'b_hq_a6_Index_bw_miss', 'b_hq_d_Index_auto_cat2', 'b_hq_d_Index_auto_cat3', 'b_hq_d_Index_auto_cat4', 'b_hq_d_Index_auto_cat5', 'b_hq_d_Index_egal_cat2', 'b_hq_d_Index_egal_cat3', 'b_hq_d_Index_egal_cat4', 'b_hq_d_Index_egal_cat5', 'b_hq_e7_h', 'b_gq_age17', 'b_gq_age16', 'b_gq_age15', 'b_gq_age14', 'b_gq_age13', 'b_gq_age12', 'b_gq_age11', 'b_gq_age10', 'b_gq_c1_ever', 'b_gq_c16', 'b_gq_c26', 'b_gq_c29', 'b_gq_d42', 'b_gq_e7a_cat2m', 'b_gq_e7a_cat3m', 'b_gq_e12a_2', 'b_gq_e12a_3', 'b_gq_e12a_4', 'b_gq_e12a_5', 'b_gq_e12a_6', 'b_gq_e12a_7', 'b_gq_e12a_8', 'b_gq_g_Index_auto_cat2', 'b_gq_g_Index_auto_cat3', 'b_gq_g_Index_auto_cat4', 'b_gq_g_Index_egal_cat2', 'b_gq_g_Index_egal_cat3', 'b_gq_g_Index_egal_cat4', 'b_gq_k7_h', 'b_gq_k7_l', 'b_hh_size', 'b_hq_hh_b6', 'b_hq_hh_b9_30_35', 'b_hq_hh_b9_35_40', 'b_hq_hh_b9_40_45', 'b_hq_hh_b9_45_50', 'b_hq_hh_b9_50_55', 'b_hq_hh_b9_55_60', 'b_hq_hh_b9_60_65', 'b_hq_hh_b9_65_70', 'b_hq_hh_b9_70', 'b_hq_hh_b12a_0', 'b_hq_hh_b12a_missing', 'b_hq_hh_b10_poly', 'b_hq_hh_b10_wido', 'b_hq_hh_b17_Kanouri', 'b_hq_hh_b17_Peul', 'b_hq_hh_b17_Touareg', 'b_hq_hh_b17_Other', 'b_hq_hh_b19_1', 'b_hq_hh_b19_2', 'b_hq_hh_b19_3', 'b_hq_hh_b19_4', 'b_hq_hh_b19_5', 'b_hq_hh_b19_6', 'b_hq_hh_b18_muslim', 'b_hq_a6f_dum', 'b_hq_a6g_dum', 'b_gq_b_a_any', 'b_gq_c_40_all_16', 'b_gq_c_40_all_17', 'b_gq_c_40_all_18', 'b_gq_c_40_all_19', 'b_gq_c_40_all_20', 'b_gq_c_40_all_21', 'b_gq_c_40_all_22', 'b_gq_c_40_all_23', 'b_gq_c_40_all_24', 'b_gq_c_40_all_25', 'b_gq_c_40_all_26', 'b_gq_c_40_all_27', 'b_gq_c_40_all_28', 'b_gq_c_40_all_29', 'b_gq_c_40_all_30', 'b_gq_c_42_all', 'b_gq_d_knowledge', 'b_gq_f_control_cat2', 'b_gq_f_control_cat3', 'b_gq_f_control_cat4', 'b_gq_f_efficacite_cat2', 'b_gq_f_efficacite_cat3', 'b_gq_f_efficacite_cat4', 'b_gq_f_estime_low', 'b_gq_f_estime_ave', 'b_gq_f_estime_hig', 'b_gq_f_estime_very_hig', 'b_gq_j_sp_cat2', 'b_gq_j_sp_cat3', 'b_gq_j_sp_cat4', 'b_admin_moyenne_cat2', 'b_admin_moyenne_cat3', 'b_admin_moyenne_cat4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9b493f3-9da6-430e-9993-df2e0b057ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ISLP\n",
      "  Downloading ISLP-0.4.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: numpy>=1.7.1 in /opt/python/lib/python3.13/site-packages (from ISLP) (2.3.4)\n",
      "Requirement already satisfied: scipy>=0.9 in /opt/python/lib/python3.13/site-packages (from ISLP) (1.16.3)\n",
      "Requirement already satisfied: pandas>=0.20 in /opt/python/lib/python3.13/site-packages (from ISLP) (2.3.3)\n",
      "Collecting lxml (from ISLP)\n",
      "  Downloading lxml-6.0.2-cp313-cp313-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: scikit-learn>=1.2 in /opt/python/lib/python3.13/site-packages (from ISLP) (1.7.2)\n",
      "Requirement already satisfied: joblib in /opt/python/lib/python3.13/site-packages (from ISLP) (1.5.2)\n",
      "Requirement already satisfied: statsmodels>=0.13 in /opt/python/lib/python3.13/site-packages (from ISLP) (0.14.5)\n",
      "Collecting lifelines (from ISLP)\n",
      "  Downloading lifelines-0.30.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting pygam (from ISLP)\n",
      "  Downloading pygam-0.10.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting torch (from ISLP)\n",
      "  Downloading torch-2.9.1-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting pytorch-lightning (from ISLP)\n",
      "  Downloading pytorch_lightning-2.5.6-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting torchmetrics (from ISLP)\n",
      "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/python/lib/python3.13/site-packages (from pandas>=0.20->ISLP) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/python/lib/python3.13/site-packages (from pandas>=0.20->ISLP) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/python/lib/python3.13/site-packages (from pandas>=0.20->ISLP) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/python/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas>=0.20->ISLP) (1.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/python/lib/python3.13/site-packages (from scikit-learn>=1.2->ISLP) (3.6.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /opt/python/lib/python3.13/site-packages (from statsmodels>=0.13->ISLP) (1.0.2)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/python/lib/python3.13/site-packages (from statsmodels>=0.13->ISLP) (25.0)\n",
      "Requirement already satisfied: matplotlib>=3.0 in /opt/python/lib/python3.13/site-packages (from lifelines->ISLP) (3.10.7)\n",
      "Collecting autograd>=1.5 (from lifelines->ISLP)\n",
      "  Downloading autograd-1.8.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting autograd-gamma>=0.3 (from lifelines->ISLP)\n",
      "  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting formulaic>=0.2.2 (from lifelines->ISLP)\n",
      "  Downloading formulaic-1.2.1-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting interface-meta>=1.2.0 (from formulaic>=0.2.2->lifelines->ISLP)\n",
      "  Downloading interface_meta-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: narwhals>=1.17 in /opt/python/lib/python3.13/site-packages (from formulaic>=0.2.2->lifelines->ISLP) (2.10.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/python/lib/python3.13/site-packages (from formulaic>=0.2.2->lifelines->ISLP) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.17.0rc1 in /opt/python/lib/python3.13/site-packages (from formulaic>=0.2.2->lifelines->ISLP) (1.17.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/python/lib/python3.13/site-packages (from matplotlib>=3.0->lifelines->ISLP) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/python/lib/python3.13/site-packages (from matplotlib>=3.0->lifelines->ISLP) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/python/lib/python3.13/site-packages (from matplotlib>=3.0->lifelines->ISLP) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/python/lib/python3.13/site-packages (from matplotlib>=3.0->lifelines->ISLP) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /opt/python/lib/python3.13/site-packages (from matplotlib>=3.0->lifelines->ISLP) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /opt/python/lib/python3.13/site-packages (from matplotlib>=3.0->lifelines->ISLP) (3.2.5)\n",
      "Collecting progressbar2<5,>=4.2.0 (from pygam->ISLP)\n",
      "  Downloading progressbar2-4.5.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting python-utils>=3.8.1 (from progressbar2<5,>=4.2.0->pygam->ISLP)\n",
      "  Downloading python_utils-3.9.1-py2.py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /opt/python/lib/python3.13/site-packages (from pytorch-lightning->ISLP) (4.67.1)\n",
      "Requirement already satisfied: PyYAML>5.4 in /opt/python/lib/python3.13/site-packages (from pytorch-lightning->ISLP) (6.0.3)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /opt/python/lib/python3.13/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (2025.10.0)\n",
      "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning->ISLP)\n",
      "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/python/lib/python3.13/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (3.13.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/python/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (1.22.0)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/python/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (3.11)\n",
      "Requirement already satisfied: setuptools in /opt/python/lib/python3.13/site-packages (from lightning-utilities>=0.10.0->pytorch-lightning->ISLP) (80.9.0)\n",
      "Collecting filelock (from torch->ISLP)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting sympy>=1.13.3 (from torch->ISLP)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch->ISLP)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/python/lib/python3.13/site-packages (from torch->ISLP) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch->ISLP)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch->ISLP)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch->ISLP)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch->ISLP)\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch->ISLP)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch->ISLP)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch->ISLP)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch->ISLP)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch->ISLP)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch->ISLP)\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch->ISLP)\n",
      "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch->ISLP)\n",
      "  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch->ISLP)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch->ISLP)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch->ISLP)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.5.1 (from torch->ISLP)\n",
      "  Downloading triton-3.5.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch->ISLP)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/python/lib/python3.13/site-packages (from jinja2->torch->ISLP) (3.0.3)\n",
      "Downloading ISLP-0.4.0-py3-none-any.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lifelines-0.30.0-py3-none-any.whl (349 kB)\n",
      "Downloading autograd-1.8.0-py3-none-any.whl (51 kB)\n",
      "Downloading formulaic-1.2.1-py3-none-any.whl (117 kB)\n",
      "Downloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n",
      "Downloading lxml-6.0.2-cp313-cp313-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pygam-0.10.1-py3-none-any.whl (80 kB)\n",
      "Downloading progressbar2-4.5.0-py3-none-any.whl (57 kB)\n",
      "Downloading python_utils-3.9.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading pytorch_lightning-2.5.6-py3-none-any.whl (831 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m831.6/831.6 kB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
      "Downloading torch-2.9.1-cp313-cp313-manylinux_2_28_x86_64.whl (899.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m  \u001b[33m0:00:10\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m  \u001b[33m0:00:10\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.5.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Building wheels for collected packages: autograd-gamma\n",
      "  Building wheel for autograd-gamma (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4119 sha256=7356eee9284a39957aefb119dc059f4e6c01c7a08e8b77e198413639c2f1b851\n",
      "  Stored in directory: /home/onyxia/.cache/pip/wheels/7e/16/46/9477f188924292d3bf1fb8fb42844201591abfc19b7ba6d868\n",
      "Successfully built autograd-gamma\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, triton, sympy, python-utils, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, lxml, lightning-utilities, interface-meta, filelock, autograd, progressbar2, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, autograd-gamma, pygam, nvidia-cusolver-cu12, formulaic, torch, lifelines, torchmetrics, pytorch-lightning, ISLP\n",
      "\u001b[2K  Attempting uninstall: nvidia-nccl-cu12━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/34\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.28.7━━━━━━\u001b[0m \u001b[32m 7/34\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.28.7:━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/34\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.28.7━━━━━━━━━━━━\u001b[0m \u001b[32m 8/34\u001b[0m [nvidia-nccl-cu12]]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34/34\u001b[0m [ISLP]2m33/34\u001b[0m [ISLP]ch-lightning]12]2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed ISLP-0.4.0 autograd-1.8.0 autograd-gamma-0.5.0 filelock-3.20.0 formulaic-1.2.1 interface-meta-1.3.0 lifelines-0.30.0 lightning-utilities-0.15.2 lxml-6.0.2 mpmath-1.3.0 networkx-3.5 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 progressbar2-4.5.0 pygam-0.10.1 python-utils-3.9.1 pytorch-lightning-2.5.6 sympy-1.14.0 torch-2.9.1 torchmetrics-1.8.2 triton-3.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install ISLP\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import \\\n",
    "     (cross_validate,\n",
    "      KFold,\n",
    "      ShuffleSplit)\n",
    "from ISLP.models import sklearn_sm\n",
    "import sklearn.model_selection as skm\n",
    "import sklearn.linear_model as skl\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cce22fd-3eaa-45bd-8267-a1fd4509318b",
   "metadata": {},
   "source": [
    "**1. Load the GHMS_2024.dta dataset in a dataframe and describe it. Hint: you will need to use the read_stata function of pandas to load the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aeb4c6b7-d7df-45ec-8458-380ae227ff4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hh_id</th>\n",
       "      <th>sample</th>\n",
       "      <th>clus</th>\n",
       "      <th>T100</th>\n",
       "      <th>C100</th>\n",
       "      <th>m_gq_dropout</th>\n",
       "      <th>m_gq_Married</th>\n",
       "      <th>b_hq_a2_wood</th>\n",
       "      <th>b_hq_a2_stone</th>\n",
       "      <th>b_hq_a2_brick</th>\n",
       "      <th>...</th>\n",
       "      <th>b_gq_f_estime_ave</th>\n",
       "      <th>b_gq_f_estime_hig</th>\n",
       "      <th>b_gq_f_estime_very_hig</th>\n",
       "      <th>b_gq_j_sp_cat1</th>\n",
       "      <th>b_gq_j_sp_cat2</th>\n",
       "      <th>b_gq_j_sp_cat3</th>\n",
       "      <th>b_gq_j_sp_cat4</th>\n",
       "      <th>b_admin_moyenne_cat2</th>\n",
       "      <th>b_admin_moyenne_cat3</th>\n",
       "      <th>b_admin_moyenne_cat4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1344.000000</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>1344.000000</td>\n",
       "      <td>1344.000000</td>\n",
       "      <td>1344.000000</td>\n",
       "      <td>1344.000000</td>\n",
       "      <td>1344.000000</td>\n",
       "      <td>1344.000000</td>\n",
       "      <td>1344.000000</td>\n",
       "      <td>1344.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>1344.000000</td>\n",
       "      <td>1344.000000</td>\n",
       "      <td>1344.000000</td>\n",
       "      <td>1344.000000</td>\n",
       "      <td>1344.000000</td>\n",
       "      <td>1344.000000</td>\n",
       "      <td>1344.000000</td>\n",
       "      <td>1344.000000</td>\n",
       "      <td>1344.000000</td>\n",
       "      <td>1344.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2186.673363</td>\n",
       "      <td>1.0</td>\n",
       "      <td>144.235119</td>\n",
       "      <td>0.505952</td>\n",
       "      <td>0.494048</td>\n",
       "      <td>0.297619</td>\n",
       "      <td>0.107887</td>\n",
       "      <td>0.123512</td>\n",
       "      <td>0.273065</td>\n",
       "      <td>0.05878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.316964</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.158482</td>\n",
       "      <td>0.409970</td>\n",
       "      <td>0.272321</td>\n",
       "      <td>0.159226</td>\n",
       "      <td>0.272321</td>\n",
       "      <td>0.200149</td>\n",
       "      <td>0.122024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>685.601257</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.506538</td>\n",
       "      <td>0.500151</td>\n",
       "      <td>0.500151</td>\n",
       "      <td>0.457381</td>\n",
       "      <td>0.310353</td>\n",
       "      <td>0.329146</td>\n",
       "      <td>0.445700</td>\n",
       "      <td>0.23530</td>\n",
       "      <td>...</td>\n",
       "      <td>0.465467</td>\n",
       "      <td>0.423765</td>\n",
       "      <td>0.047210</td>\n",
       "      <td>0.365328</td>\n",
       "      <td>0.492011</td>\n",
       "      <td>0.445320</td>\n",
       "      <td>0.366023</td>\n",
       "      <td>0.445320</td>\n",
       "      <td>0.400260</td>\n",
       "      <td>0.327435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1001.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1597.750000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2162.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2814.250000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>215.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3366.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>284.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 144 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             hh_id  sample         clus         T100         C100  \\\n",
       "count  1344.000000  1344.0  1344.000000  1344.000000  1344.000000   \n",
       "mean   2186.673363     1.0   144.235119     0.505952     0.494048   \n",
       "std     685.601257     0.0    84.506538     0.500151     0.500151   \n",
       "min    1001.000000     1.0     1.000000     0.000000     0.000000   \n",
       "25%    1597.750000     1.0    66.000000     0.000000     0.000000   \n",
       "50%    2162.500000     1.0   146.000000     1.000000     0.000000   \n",
       "75%    2814.250000     1.0   215.000000     1.000000     1.000000   \n",
       "max    3366.000000     1.0   284.000000     1.000000     1.000000   \n",
       "\n",
       "       m_gq_dropout  m_gq_Married  b_hq_a2_wood  b_hq_a2_stone  b_hq_a2_brick  \\\n",
       "count   1344.000000   1344.000000   1344.000000    1344.000000     1344.00000   \n",
       "mean       0.297619      0.107887      0.123512       0.273065        0.05878   \n",
       "std        0.457381      0.310353      0.329146       0.445700        0.23530   \n",
       "min        0.000000      0.000000      0.000000       0.000000        0.00000   \n",
       "25%        0.000000      0.000000      0.000000       0.000000        0.00000   \n",
       "50%        0.000000      0.000000      0.000000       0.000000        0.00000   \n",
       "75%        1.000000      0.000000      0.000000       1.000000        0.00000   \n",
       "max        1.000000      1.000000      1.000000       1.000000        1.00000   \n",
       "\n",
       "       ...  b_gq_f_estime_ave  b_gq_f_estime_hig  b_gq_f_estime_very_hig  \\\n",
       "count  ...        1344.000000        1344.000000             1344.000000   \n",
       "mean   ...           0.316964           0.234375                0.002232   \n",
       "std    ...           0.465467           0.423765                0.047210   \n",
       "min    ...           0.000000           0.000000                0.000000   \n",
       "25%    ...           0.000000           0.000000                0.000000   \n",
       "50%    ...           0.000000           0.000000                0.000000   \n",
       "75%    ...           1.000000           0.000000                0.000000   \n",
       "max    ...           1.000000           1.000000                1.000000   \n",
       "\n",
       "       b_gq_j_sp_cat1  b_gq_j_sp_cat2  b_gq_j_sp_cat3  b_gq_j_sp_cat4  \\\n",
       "count     1344.000000     1344.000000     1344.000000     1344.000000   \n",
       "mean         0.158482        0.409970        0.272321        0.159226   \n",
       "std          0.365328        0.492011        0.445320        0.366023   \n",
       "min          0.000000        0.000000        0.000000        0.000000   \n",
       "25%          0.000000        0.000000        0.000000        0.000000   \n",
       "50%          0.000000        0.000000        0.000000        0.000000   \n",
       "75%          0.000000        1.000000        1.000000        0.000000   \n",
       "max          1.000000        1.000000        1.000000        1.000000   \n",
       "\n",
       "       b_admin_moyenne_cat2  b_admin_moyenne_cat3  b_admin_moyenne_cat4  \n",
       "count           1344.000000           1344.000000           1344.000000  \n",
       "mean               0.272321              0.200149              0.122024  \n",
       "std                0.445320              0.400260              0.327435  \n",
       "min                0.000000              0.000000              0.000000  \n",
       "25%                0.000000              0.000000              0.000000  \n",
       "50%                0.000000              0.000000              0.000000  \n",
       "75%                1.000000              0.000000              0.000000  \n",
       "max                1.000000              1.000000              1.000000  \n",
       "\n",
       "[8 rows x 144 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ghms = pd.read_stata('GHMS_2024.dta')\n",
    "ghms.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141fa46e-acc8-4d98-97ca-cad9e52b77d9",
   "metadata": {},
   "source": [
    "**2. Using the sub-sample of girls assigned to the control group (C100 = 1) and the validation set approach, predict the school enrollment status of Nigerian girls approximately three years after they enter middle school (endline enrollment status) using a logit model. Use all available baseline covariates as predictors. Use a seed of 3. Hint: you will need to drop a few variables which do not make sense as predictors from the list of predictors (such as the unique IDs of individuals and households, you can also drop the strata).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b72b13af-e8b6-4452-ac64-99942e24a658",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = ['b_hq_a2_wood', 'b_hq_a2_stone', 'b_hq_a2_brick', 'b_hq_a2_cement', 'b_hq_a6_Index_bw_Q2', 'b_hq_a6_Index_bw_Q3', 'b_hq_a6_Index_bw_Q4', 'b_hq_a6_Index_bw_miss', 'b_hq_d_Index_auto_cat2', 'b_hq_d_Index_auto_cat3', 'b_hq_d_Index_auto_cat4', 'b_hq_d_Index_auto_cat5', 'b_hq_d_Index_egal_cat2', 'b_hq_d_Index_egal_cat3', 'b_hq_d_Index_egal_cat4', 'b_hq_d_Index_egal_cat5', 'b_hq_e7_h', 'b_hq_e7_m', 'b_gq_agem', 'b_gq_age17', 'b_gq_age16', 'b_gq_age15', 'b_gq_age14', 'b_gq_age13', 'b_gq_age12', 'b_gq_age11', 'b_gq_age10', 'b_gq_c1_ever', 'b_gq_c16', 'b_gq_c26', 'b_gq_c29', 'b_gq_d42', 'b_gq_e7a_m', 'b_gq_e7a_cat2m', 'b_gq_e7a_cat3m', 'b_gq_e12a_2', 'b_gq_e12a_3', 'b_gq_e12a_4', 'b_gq_e12a_5', 'b_gq_e12a_6', 'b_gq_e12a_7', 'b_gq_e12a_8', 'b_gq_g_Index_auto_cat2', 'b_gq_g_Index_auto_cat3', 'b_gq_g_Index_auto_cat4', 'b_gq_g_Index_egal_cat2', 'b_gq_g_Index_egal_cat3', 'b_gq_g_Index_egal_cat4', 'b_gq_k7_h', 'b_gq_k7_l', 'b_gq_k7_m', 'b_hh_size', 'b_hq_hh_b6', 'b_hq_hh_b6_missing', 'b_hq_hh_b9_30_35', 'b_hq_hh_b9_35_40', 'b_hq_hh_b9_40_45', 'b_hq_hh_b9_45_50', 'b_hq_hh_b9_50_55', 'b_hq_hh_b9_55_60', 'b_hq_hh_b9_60_65', 'b_hq_hh_b9_65_70', 'b_hq_hh_b9_70', 'b_hq_hh_b9_m', 'b_hq_hh_b12a_0', 'b_hq_hh_b12a_missing', 'b_hq_hh_b10_poly', 'b_hq_hh_b10_wido', 'b_hq_hh_b10_missing', 'b_hq_hh_b17_Kanouri', 'b_hq_hh_b17_Peul', 'b_hq_hh_b17_Touareg', 'b_hq_hh_b17_Other', 'b_hq_hh_b17_Missing', 'b_hq_hh_b19_1', 'b_hq_hh_b19_2', 'b_hq_hh_b19_3', 'b_hq_hh_b19_4', 'b_hq_hh_b19_5', 'b_hq_hh_b19_6', 'b_hq_hh_b18_muslim', 'b_hq_hh_b18_missing', 'b_hq_a6f_dum', 'b_hq_a6g_dum', 'b_gq_b_a_any', 'b_gq_c_40_all_m', 'b_gq_c_40_all_16', 'b_gq_c_40_all_17', 'b_gq_c_40_all_18', 'b_gq_c_40_all_19', 'b_gq_c_40_all_20', 'b_gq_c_40_all_21', 'b_gq_c_40_all_22', 'b_gq_c_40_all_23', 'b_gq_c_40_all_24', 'b_gq_c_40_all_25', 'b_gq_c_40_all_26', 'b_gq_c_40_all_27', 'b_gq_c_40_all_28', 'b_gq_c_40_all_29', 'b_gq_c_40_all_30', 'b_gq_c_42_all', 'b_gq_d_knowledge', 'b_gq_f_control_cat2', 'b_gq_f_control_cat3', 'b_gq_f_control_cat4', 'b_gq_f_efficacite_cat2', 'b_gq_f_efficacite_cat3', 'b_gq_f_efficacite_cat4', 'b_gq_f_estime_low', 'b_gq_f_estime_ave', 'b_gq_f_estime_hig', 'b_gq_f_estime_very_hig', 'b_gq_j_sp_cat2', 'b_gq_j_sp_cat3', 'b_gq_j_sp_cat4', 'b_admin_moyenne_cat2', 'b_admin_moyenne_cat3', 'b_admin_moyenne_cat4']\n",
    "\n",
    "features = ['b_hq_a2_wood', 'b_hq_a2_stone', 'b_hq_a2_brick', 'b_hq_a2_cement', 'b_hq_a6_Index_bw_Q2', 'b_hq_a6_Index_bw_Q3', 'b_hq_a6_Index_bw_Q4', 'b_hq_a6_Index_bw_miss', 'b_hq_d_Index_auto_cat2', 'b_hq_d_Index_auto_cat3', 'b_hq_d_Index_auto_cat4', 'b_hq_d_Index_auto_cat5', 'b_hq_d_Index_egal_cat2', 'b_hq_d_Index_egal_cat3', 'b_hq_d_Index_egal_cat4', 'b_hq_d_Index_egal_cat5', 'b_hq_e7_h', 'b_gq_age17', 'b_gq_age16', 'b_gq_age15', 'b_gq_age14', 'b_gq_age13', 'b_gq_age12', 'b_gq_age11', 'b_gq_age10', 'b_gq_c1_ever', 'b_gq_c16', 'b_gq_c26', 'b_gq_c29', 'b_gq_d42', 'b_gq_e7a_cat2m', 'b_gq_e7a_cat3m', 'b_gq_e12a_2', 'b_gq_e12a_3', 'b_gq_e12a_4', 'b_gq_e12a_5', 'b_gq_e12a_6', 'b_gq_e12a_7', 'b_gq_e12a_8', 'b_gq_g_Index_auto_cat2', 'b_gq_g_Index_auto_cat3', 'b_gq_g_Index_auto_cat4', 'b_gq_g_Index_egal_cat2', 'b_gq_g_Index_egal_cat3', 'b_gq_g_Index_egal_cat4', 'b_gq_k7_h', 'b_gq_k7_l', 'b_hh_size', 'b_hq_hh_b6', 'b_hq_hh_b9_30_35', 'b_hq_hh_b9_35_40', 'b_hq_hh_b9_40_45', 'b_hq_hh_b9_45_50', 'b_hq_hh_b9_50_55', 'b_hq_hh_b9_55_60', 'b_hq_hh_b9_60_65', 'b_hq_hh_b9_65_70', 'b_hq_hh_b9_70', 'b_hq_hh_b12a_0', 'b_hq_hh_b12a_missing', 'b_hq_hh_b10_poly', 'b_hq_hh_b10_wido', 'b_hq_hh_b17_Kanouri', 'b_hq_hh_b17_Peul', 'b_hq_hh_b17_Touareg', 'b_hq_hh_b17_Other', 'b_hq_hh_b19_1', 'b_hq_hh_b19_2', 'b_hq_hh_b19_3', 'b_hq_hh_b19_4', 'b_hq_hh_b19_5', 'b_hq_hh_b19_6', 'b_hq_hh_b18_muslim', 'b_hq_a6f_dum', 'b_hq_a6g_dum', 'b_gq_b_a_any', 'b_gq_c_40_all_16', 'b_gq_c_40_all_17', 'b_gq_c_40_all_18', 'b_gq_c_40_all_19', 'b_gq_c_40_all_20', 'b_gq_c_40_all_21', 'b_gq_c_40_all_22', 'b_gq_c_40_all_23', 'b_gq_c_40_all_24', 'b_gq_c_40_all_25', 'b_gq_c_40_all_26', 'b_gq_c_40_all_27', 'b_gq_c_40_all_28', 'b_gq_c_40_all_29', 'b_gq_c_40_all_30', 'b_gq_c_42_all', 'b_gq_d_knowledge', 'b_gq_f_control_cat2', 'b_gq_f_control_cat3', 'b_gq_f_control_cat4', 'b_gq_f_efficacite_cat2', 'b_gq_f_efficacite_cat3', 'b_gq_f_efficacite_cat4', 'b_gq_f_estime_low', 'b_gq_f_estime_ave', 'b_gq_f_estime_hig', 'b_gq_f_estime_very_hig', 'b_gq_j_sp_cat2', 'b_gq_j_sp_cat3', 'b_gq_j_sp_cat4', 'b_admin_moyenne_cat2', 'b_admin_moyenne_cat3', 'b_admin_moyenne_cat4']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d841735-ded3-4d10-aa16-b8bf3e50a1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter dataframe and select relevant columns\n",
    "filt = ghms['C100'] == 1\n",
    "df = ghms.loc[filt, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a8754d8-65b7-497e-b840-582bc0583469",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create response vector Y and matrix of predictors X\n",
    "\n",
    "df_train, df_test = train_test_split(df,\n",
    "                                         test_size=0.25,\n",
    "                                         random_state=3)\n",
    "\n",
    "Y_train, Y_test = df_train['m_gq_dropout'], df_test['m_gq_dropout']\n",
    "\n",
    "\n",
    "X_train, X_test = df_train[features], df_test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1909a4d-7a5d-41cc-9a30-9ca2fb3314df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/python/lib/python3.13/site-packages/statsmodels/genmod/families/links.py:198: RuntimeWarning: overflow encountered in exp\n",
      "  t = np.exp(-z)\n",
      "/opt/python/lib/python3.13/site-packages/statsmodels/genmod/families/family.py:1056: RuntimeWarning: divide by zero encountered in log\n",
      "  special.gammaln(n - y + 1) + y * np.log(mu / (1 - mu + 1e-20)) +\n",
      "/opt/python/lib/python3.13/site-packages/statsmodels/genmod/families/family.py:1056: RuntimeWarning: invalid value encountered in multiply\n",
      "  special.gammaln(n - y + 1) + y * np.log(mu / (1 - mu + 1e-20)) +\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>m_gq_dropout</td>   <th>  No. Observations:  </th>  <td>   498</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td>   396</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>        <td>Binomial</td>     <th>  Df Model:          </th>  <td>   101</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>         <td>Logit</td>      <th>  Scale:             </th> <td>  1.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td>     nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Fri, 14 Nov 2025</td> <th>  Deviance:          </th> <td>  15289.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>17:52:25</td>     <th>  Pearson chi2:      </th> <td>7.48e+17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>         <td>79</td>        <th>  Pseudo R-squ. (CS):</th>  <td>   nan</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td> 3.894e+14</td> <td> 3.59e+06</td> <td> 1.09e+08</td> <td> 0.000</td> <td> 3.89e+14</td> <td> 3.89e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td> 1.426e+14</td> <td> 3.61e+06</td> <td> 3.95e+07</td> <td> 0.000</td> <td> 1.43e+14</td> <td> 1.43e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td> 1.504e+14</td> <td> 3.58e+06</td> <td>  4.2e+07</td> <td> 0.000</td> <td>  1.5e+14</td> <td>  1.5e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>-9.073e+13</td> <td> 3.52e+06</td> <td>-2.58e+07</td> <td> 0.000</td> <td>-9.07e+13</td> <td>-9.07e+13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td> 9.862e+14</td> <td> 4.09e+06</td> <td> 2.41e+08</td> <td> 0.000</td> <td> 9.86e+14</td> <td> 9.86e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td> 3.603e+14</td> <td> 4.34e+06</td> <td> 8.29e+07</td> <td> 0.000</td> <td>  3.6e+14</td> <td>  3.6e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>-1.437e+14</td> <td> 4.61e+06</td> <td>-3.12e+07</td> <td> 0.000</td> <td>-1.44e+14</td> <td>-1.44e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    1.5487</td> <td> 2.31e-08</td> <td>  6.7e+07</td> <td> 0.000</td> <td>    1.549</td> <td>    1.549</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>-7.347e+14</td> <td> 1.39e+07</td> <td>-5.28e+07</td> <td> 0.000</td> <td>-7.35e+14</td> <td>-7.35e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td> 1.055e+15</td> <td> 1.87e+07</td> <td> 5.63e+07</td> <td> 0.000</td> <td> 1.06e+15</td> <td> 1.06e+15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td> 6.401e+14</td> <td> 1.73e+07</td> <td> 3.69e+07</td> <td> 0.000</td> <td>  6.4e+14</td> <td>  6.4e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>    4.6450</td> <td> 2.57e-08</td> <td> 1.81e+08</td> <td> 0.000</td> <td>    4.645</td> <td>    4.645</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td>   5.6e+14</td> <td> 3.79e+06</td> <td> 1.48e+08</td> <td> 0.000</td> <td>  5.6e+14</td> <td>  5.6e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td> 2.723e+14</td> <td> 3.66e+06</td> <td> 7.43e+07</td> <td> 0.000</td> <td> 2.72e+14</td> <td> 2.72e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td>-1.533e+14</td> <td> 3.46e+06</td> <td>-4.43e+07</td> <td> 0.000</td> <td>-1.53e+14</td> <td>-1.53e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td>   -0.6957</td> <td> 3.93e-08</td> <td>-1.77e+07</td> <td> 0.000</td> <td>   -0.696</td> <td>   -0.696</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td> 1.005e+15</td> <td> 3.46e+06</td> <td> 2.91e+08</td> <td> 0.000</td> <td> 1.01e+15</td> <td> 1.01e+15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>   <td> 4.461e+15</td> <td> 5.37e+06</td> <td> 8.31e+08</td> <td> 0.000</td> <td> 4.46e+15</td> <td> 4.46e+15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>   <td>  3.12e+13</td> <td>  8.4e+06</td> <td> 3.71e+06</td> <td> 0.000</td> <td> 3.12e+13</td> <td> 3.12e+13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>   <td> -5.98e+14</td> <td> 1.14e+07</td> <td>-5.22e+07</td> <td> 0.000</td> <td>-5.98e+14</td> <td>-5.98e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>   <td>-6.433e+14</td> <td> 1.91e+07</td> <td>-3.38e+07</td> <td> 0.000</td> <td>-6.43e+14</td> <td>-6.43e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>   <td> 4.152e+14</td> <td> 2.09e+07</td> <td> 1.99e+07</td> <td> 0.000</td> <td> 4.15e+14</td> <td> 4.15e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>   <td> 6.722e+14</td> <td> 1.74e+07</td> <td> 3.86e+07</td> <td> 0.000</td> <td> 6.72e+14</td> <td> 6.72e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>   <td> 2.039e+14</td> <td> 8.47e+06</td> <td> 2.41e+07</td> <td> 0.000</td> <td> 2.04e+14</td> <td> 2.04e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>   <td>-3.282e+14</td> <td> 3.86e+06</td> <td> -8.5e+07</td> <td> 0.000</td> <td>-3.28e+14</td> <td>-3.28e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>   <td> 8.484e+15</td> <td> 3.25e+06</td> <td> 2.61e+09</td> <td> 0.000</td> <td> 8.48e+15</td> <td> 8.48e+15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>   <td> 4.264e+14</td> <td> 3.45e+06</td> <td> 1.23e+08</td> <td> 0.000</td> <td> 4.26e+14</td> <td> 4.26e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>   <td>-7.088e+14</td> <td> 4.37e+06</td> <td>-1.62e+08</td> <td> 0.000</td> <td>-7.09e+14</td> <td>-7.09e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>   <td> 2.137e+14</td> <td> 3.59e+06</td> <td> 5.95e+07</td> <td> 0.000</td> <td> 2.14e+14</td> <td> 2.14e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th>   <td> 2.402e+14</td> <td> 3.79e+06</td> <td> 6.33e+07</td> <td> 0.000</td> <td>  2.4e+14</td> <td>  2.4e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th>   <td>-9.225e+14</td> <td> 4.93e+06</td> <td>-1.87e+08</td> <td> 0.000</td> <td>-9.22e+14</td> <td>-9.22e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th>   <td>-5.008e+14</td> <td> 5.44e+06</td> <td> -9.2e+07</td> <td> 0.000</td> <td>-5.01e+14</td> <td>-5.01e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x32</th>   <td> 6.854e+14</td> <td> 4.77e+06</td> <td> 1.44e+08</td> <td> 0.000</td> <td> 6.85e+14</td> <td> 6.85e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33</th>   <td> 9.775e+14</td> <td> 7.36e+06</td> <td> 1.33e+08</td> <td> 0.000</td> <td> 9.78e+14</td> <td> 9.78e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x34</th>   <td> 1.558e+15</td> <td> 1.33e+07</td> <td> 1.17e+08</td> <td> 0.000</td> <td> 1.56e+15</td> <td> 1.56e+15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x35</th>   <td> 1.137e+15</td> <td> 1.15e+07</td> <td> 9.92e+07</td> <td> 0.000</td> <td> 1.14e+15</td> <td> 1.14e+15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x36</th>   <td> 6.038e+14</td> <td> 6.27e+06</td> <td> 9.63e+07</td> <td> 0.000</td> <td> 6.04e+14</td> <td> 6.04e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x37</th>   <td> 1.131e+15</td> <td> 5.67e+06</td> <td> 1.99e+08</td> <td> 0.000</td> <td> 1.13e+15</td> <td> 1.13e+15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x38</th>   <td> 2.161e+14</td> <td> 4.94e+06</td> <td> 4.37e+07</td> <td> 0.000</td> <td> 2.16e+14</td> <td> 2.16e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x39</th>   <td>-2.376e+14</td> <td> 1.99e+07</td> <td>-1.19e+07</td> <td> 0.000</td> <td>-2.38e+14</td> <td>-2.38e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x40</th>   <td> 5.929e+12</td> <td> 3.59e+07</td> <td> 1.65e+05</td> <td> 0.000</td> <td> 5.93e+12</td> <td> 5.93e+12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x41</th>   <td>-3.116e+14</td> <td> 3.75e+07</td> <td> -8.3e+06</td> <td> 0.000</td> <td>-3.12e+14</td> <td>-3.12e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x42</th>   <td> 2.704e+14</td> <td> 3.65e+06</td> <td>  7.4e+07</td> <td> 0.000</td> <td>  2.7e+14</td> <td>  2.7e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x43</th>   <td> 8.093e+14</td> <td> 3.77e+06</td> <td> 2.15e+08</td> <td> 0.000</td> <td> 8.09e+14</td> <td> 8.09e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x44</th>   <td> 7.558e+14</td> <td> 3.49e+06</td> <td> 2.17e+08</td> <td> 0.000</td> <td> 7.56e+14</td> <td> 7.56e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x45</th>   <td>-3.222e+14</td> <td> 1.91e+06</td> <td>-1.69e+08</td> <td> 0.000</td> <td>-3.22e+14</td> <td>-3.22e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x46</th>   <td> 3.222e+14</td> <td> 1.91e+06</td> <td> 1.69e+08</td> <td> 0.000</td> <td> 3.22e+14</td> <td> 3.22e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x47</th>   <td>-2.411e+14</td> <td>  4.1e+06</td> <td>-5.88e+07</td> <td> 0.000</td> <td>-2.41e+14</td> <td>-2.41e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x48</th>   <td> 5.629e+14</td> <td> 4.67e+06</td> <td> 1.21e+08</td> <td> 0.000</td> <td> 5.63e+14</td> <td> 5.63e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x49</th>   <td> 2.481e+14</td> <td> 5.97e+06</td> <td> 4.15e+07</td> <td> 0.000</td> <td> 2.48e+14</td> <td> 2.48e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x50</th>   <td> 3.703e+14</td> <td> 7.49e+06</td> <td> 4.94e+07</td> <td> 0.000</td> <td>  3.7e+14</td> <td>  3.7e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x51</th>   <td> 8.253e+14</td> <td> 7.04e+06</td> <td> 1.17e+08</td> <td> 0.000</td> <td> 8.25e+14</td> <td> 8.25e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x52</th>   <td> 6.558e+14</td> <td> 7.16e+06</td> <td> 9.16e+07</td> <td> 0.000</td> <td> 6.56e+14</td> <td> 6.56e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x53</th>   <td>  9.72e+14</td> <td> 6.24e+06</td> <td> 1.56e+08</td> <td> 0.000</td> <td> 9.72e+14</td> <td> 9.72e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x54</th>   <td> 1.118e+15</td> <td> 7.22e+06</td> <td> 1.55e+08</td> <td> 0.000</td> <td> 1.12e+15</td> <td> 1.12e+15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x55</th>   <td> 2.039e+13</td> <td> 5.98e+06</td> <td> 3.41e+06</td> <td> 0.000</td> <td> 2.04e+13</td> <td> 2.04e+13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x56</th>   <td>-5.894e+13</td> <td> 5.42e+06</td> <td>-1.09e+07</td> <td> 0.000</td> <td>-5.89e+13</td> <td>-5.89e+13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x57</th>   <td> 5.884e+14</td> <td> 5.67e+06</td> <td> 1.04e+08</td> <td> 0.000</td> <td> 5.88e+14</td> <td> 5.88e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x58</th>   <td> -1.79e+14</td> <td> 3.79e+06</td> <td>-4.72e+07</td> <td> 0.000</td> <td>-1.79e+14</td> <td>-1.79e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x59</th>   <td> 2.823e+13</td> <td> 1.67e+06</td> <td> 1.69e+07</td> <td> 0.000</td> <td> 2.82e+13</td> <td> 2.82e+13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x60</th>   <td> 3.892e+11</td> <td> 3.97e+06</td> <td> 9.79e+04</td> <td> 0.000</td> <td> 3.89e+11</td> <td> 3.89e+11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x61</th>   <td>-3.674e+14</td> <td> 4.04e+06</td> <td>-9.09e+07</td> <td> 0.000</td> <td>-3.67e+14</td> <td>-3.67e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x62</th>   <td>-9.883e+14</td> <td> 3.48e+06</td> <td>-2.84e+08</td> <td> 0.000</td> <td>-9.88e+14</td> <td>-9.88e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x63</th>   <td>-7.009e+14</td> <td> 3.49e+06</td> <td>-2.01e+08</td> <td> 0.000</td> <td>-7.01e+14</td> <td>-7.01e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x64</th>   <td> 9.966e+14</td> <td> 3.45e+06</td> <td> 2.89e+08</td> <td> 0.000</td> <td> 9.97e+14</td> <td> 9.97e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x65</th>   <td> -5.65e+14</td> <td> 3.27e+06</td> <td>-1.73e+08</td> <td> 0.000</td> <td>-5.65e+14</td> <td>-5.65e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x66</th>   <td> 3.655e+15</td> <td>  3.2e+06</td> <td> 1.14e+09</td> <td> 0.000</td> <td> 3.65e+15</td> <td> 3.65e+15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x67</th>   <td>-3.421e+14</td> <td> 3.05e+06</td> <td>-1.12e+08</td> <td> 0.000</td> <td>-3.42e+14</td> <td>-3.42e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x68</th>   <td> -9.28e+14</td> <td> 3.32e+06</td> <td> -2.8e+08</td> <td> 0.000</td> <td>-9.28e+14</td> <td>-9.28e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x69</th>   <td>-9.125e+14</td> <td>  2.7e+06</td> <td>-3.37e+08</td> <td> 0.000</td> <td>-9.13e+14</td> <td>-9.13e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x70</th>   <td>-3.657e+14</td> <td> 1.94e+06</td> <td>-1.89e+08</td> <td> 0.000</td> <td>-3.66e+14</td> <td>-3.66e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x71</th>   <td> 2.823e+13</td> <td> 1.67e+06</td> <td> 1.69e+07</td> <td> 0.000</td> <td> 2.82e+13</td> <td> 2.82e+13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x72</th>   <td>-2.823e+13</td> <td> 1.67e+06</td> <td>-1.69e+07</td> <td> 0.000</td> <td>-2.82e+13</td> <td>-2.82e+13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x73</th>   <td> 1.078e+14</td> <td> 3.53e+06</td> <td> 3.06e+07</td> <td> 0.000</td> <td> 1.08e+14</td> <td> 1.08e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x74</th>   <td>-1.426e+15</td> <td> 3.91e+06</td> <td>-3.65e+08</td> <td> 0.000</td> <td>-1.43e+15</td> <td>-1.43e+15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x75</th>   <td> 2.345e+13</td> <td> 3.46e+06</td> <td> 6.79e+06</td> <td> 0.000</td> <td> 2.34e+13</td> <td> 2.34e+13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x76</th>   <td>-4.738e+14</td> <td> 4.35e+06</td> <td>-1.09e+08</td> <td> 0.000</td> <td>-4.74e+14</td> <td>-4.74e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x77</th>   <td> 3.031e+14</td> <td> 4.35e+06</td> <td> 6.97e+07</td> <td> 0.000</td> <td> 3.03e+14</td> <td> 3.03e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x78</th>   <td>-3.452e+14</td> <td> 5.21e+06</td> <td>-6.62e+07</td> <td> 0.000</td> <td>-3.45e+14</td> <td>-3.45e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x79</th>   <td>-4.826e+14</td> <td> 4.77e+06</td> <td>-1.01e+08</td> <td> 0.000</td> <td>-4.83e+14</td> <td>-4.83e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x80</th>   <td>-3.359e+14</td> <td> 6.74e+06</td> <td>-4.98e+07</td> <td> 0.000</td> <td>-3.36e+14</td> <td>-3.36e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x81</th>   <td> 2.364e+14</td> <td>    5e+06</td> <td> 4.73e+07</td> <td> 0.000</td> <td> 2.36e+14</td> <td> 2.36e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x82</th>   <td>-1.282e+15</td> <td> 5.08e+06</td> <td>-2.52e+08</td> <td> 0.000</td> <td>-1.28e+15</td> <td>-1.28e+15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x83</th>   <td> -2.89e+14</td> <td> 4.32e+06</td> <td> -6.7e+07</td> <td> 0.000</td> <td>-2.89e+14</td> <td>-2.89e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x84</th>   <td>-1.181e+15</td> <td> 4.29e+06</td> <td>-2.75e+08</td> <td> 0.000</td> <td>-1.18e+15</td> <td>-1.18e+15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x85</th>   <td> 4.828e+13</td> <td> 4.57e+06</td> <td> 1.06e+07</td> <td> 0.000</td> <td> 4.83e+13</td> <td> 4.83e+13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x86</th>   <td> -4.65e+15</td> <td> 3.61e+06</td> <td>-1.29e+09</td> <td> 0.000</td> <td>-4.65e+15</td> <td>-4.65e+15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x87</th>   <td>-2.322e+14</td> <td> 3.58e+06</td> <td>-6.49e+07</td> <td> 0.000</td> <td>-2.32e+14</td> <td>-2.32e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x88</th>   <td>-1.826e+16</td> <td> 3.39e+06</td> <td>-5.38e+09</td> <td> 0.000</td> <td>-1.83e+16</td> <td>-1.83e+16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x89</th>   <td>-5.126e+15</td> <td> 3.51e+06</td> <td>-1.46e+09</td> <td> 0.000</td> <td>-5.13e+15</td> <td>-5.13e+15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x90</th>   <td>  1.59e+15</td> <td> 4.44e+06</td> <td> 3.58e+08</td> <td> 0.000</td> <td> 1.59e+15</td> <td> 1.59e+15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x91</th>   <td>-1.226e+15</td> <td> 3.27e+06</td> <td>-3.75e+08</td> <td> 0.000</td> <td>-1.23e+15</td> <td>-1.23e+15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x92</th>   <td>-7.676e+14</td> <td> 3.74e+06</td> <td>-2.05e+08</td> <td> 0.000</td> <td>-7.68e+14</td> <td>-7.68e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x93</th>   <td> 4.021e+12</td> <td> 4.02e+06</td> <td>    1e+06</td> <td> 0.000</td> <td> 4.02e+12</td> <td> 4.02e+12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x94</th>   <td>-4.669e+14</td> <td>  4.1e+06</td> <td>-1.14e+08</td> <td> 0.000</td> <td>-4.67e+14</td> <td>-4.67e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x95</th>   <td> 1.246e+13</td> <td> 4.28e+06</td> <td> 2.91e+06</td> <td> 0.000</td> <td> 1.25e+13</td> <td> 1.25e+13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x96</th>   <td>-1.033e+15</td> <td> 4.13e+06</td> <td> -2.5e+08</td> <td> 0.000</td> <td>-1.03e+15</td> <td>-1.03e+15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x97</th>   <td>-1.203e+15</td> <td> 3.99e+06</td> <td>-3.02e+08</td> <td> 0.000</td> <td> -1.2e+15</td> <td> -1.2e+15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x98</th>   <td>-1.589e+15</td> <td> 4.64e+06</td> <td>-3.43e+08</td> <td> 0.000</td> <td>-1.59e+15</td> <td>-1.59e+15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x99</th>   <td>-4.923e+14</td> <td> 6.55e+06</td> <td>-7.52e+07</td> <td> 0.000</td> <td>-4.92e+14</td> <td>-4.92e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x100</th>  <td> 1.028e+14</td> <td> 6.57e+06</td> <td> 1.57e+07</td> <td> 0.000</td> <td> 1.03e+14</td> <td> 1.03e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x101</th>  <td>-1.276e+14</td> <td> 6.39e+06</td> <td>   -2e+07</td> <td> 0.000</td> <td>-1.28e+14</td> <td>-1.28e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x102</th>  <td> 2.421e+15</td> <td> 3.43e+06</td> <td> 7.06e+08</td> <td> 0.000</td> <td> 2.42e+15</td> <td> 2.42e+15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x103</th>  <td>-1.418e+14</td> <td> 4.86e+06</td> <td>-2.92e+07</td> <td> 0.000</td> <td>-1.42e+14</td> <td>-1.42e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x104</th>  <td>-1.043e+15</td> <td> 4.79e+06</td> <td>-2.18e+08</td> <td> 0.000</td> <td>-1.04e+15</td> <td>-1.04e+15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x105</th>  <td> 1.143e+15</td> <td> 4.17e+06</td> <td> 2.74e+08</td> <td> 0.000</td> <td> 1.14e+15</td> <td> 1.14e+15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x106</th>  <td> -3.73e+13</td> <td> 3.68e+06</td> <td>-1.01e+07</td> <td> 0.000</td> <td>-3.73e+13</td> <td>-3.73e+13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x107</th>  <td>-1.901e+14</td> <td> 3.72e+06</td> <td>-5.12e+07</td> <td> 0.000</td> <td> -1.9e+14</td> <td> -1.9e+14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x108</th>  <td>-1.168e+15</td> <td> 3.55e+06</td> <td> -3.3e+08</td> <td> 0.000</td> <td>-1.17e+15</td> <td>-1.17e+15</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}   &  m\\_gq\\_dropout  & \\textbf{  No. Observations:  } &      498    \\\\\n",
       "\\textbf{Model:}           &       GLM        & \\textbf{  Df Residuals:      } &      396    \\\\\n",
       "\\textbf{Model Family:}    &     Binomial     & \\textbf{  Df Model:          } &      101    \\\\\n",
       "\\textbf{Link Function:}   &      Logit       & \\textbf{  Scale:             } &    1.0000   \\\\\n",
       "\\textbf{Method:}          &       IRLS       & \\textbf{  Log-Likelihood:    } &       nan   \\\\\n",
       "\\textbf{Date:}            & Fri, 14 Nov 2025 & \\textbf{  Deviance:          } &    15289.   \\\\\n",
       "\\textbf{Time:}            &     17:52:25     & \\textbf{  Pearson chi2:      } &  7.48e+17   \\\\\n",
       "\\textbf{No. Iterations:}  &        79        & \\textbf{  Pseudo R-squ. (CS):} &      nan    \\\\\n",
       "\\textbf{Covariance Type:} &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{x1}    &    3.894e+14  &     3.59e+06     &  1.09e+08  &         0.000        &     3.89e+14    &     3.89e+14     \\\\\n",
       "\\textbf{x2}    &    1.426e+14  &     3.61e+06     &  3.95e+07  &         0.000        &     1.43e+14    &     1.43e+14     \\\\\n",
       "\\textbf{x3}    &    1.504e+14  &     3.58e+06     &   4.2e+07  &         0.000        &      1.5e+14    &      1.5e+14     \\\\\n",
       "\\textbf{x4}    &   -9.073e+13  &     3.52e+06     & -2.58e+07  &         0.000        &    -9.07e+13    &    -9.07e+13     \\\\\n",
       "\\textbf{x5}    &    9.862e+14  &     4.09e+06     &  2.41e+08  &         0.000        &     9.86e+14    &     9.86e+14     \\\\\n",
       "\\textbf{x6}    &    3.603e+14  &     4.34e+06     &  8.29e+07  &         0.000        &      3.6e+14    &      3.6e+14     \\\\\n",
       "\\textbf{x7}    &   -1.437e+14  &     4.61e+06     & -3.12e+07  &         0.000        &    -1.44e+14    &    -1.44e+14     \\\\\n",
       "\\textbf{const} &       1.5487  &     2.31e-08     &   6.7e+07  &         0.000        &        1.549    &        1.549     \\\\\n",
       "\\textbf{x8}    &   -7.347e+14  &     1.39e+07     & -5.28e+07  &         0.000        &    -7.35e+14    &    -7.35e+14     \\\\\n",
       "\\textbf{x9}    &    1.055e+15  &     1.87e+07     &  5.63e+07  &         0.000        &     1.06e+15    &     1.06e+15     \\\\\n",
       "\\textbf{x10}   &    6.401e+14  &     1.73e+07     &  3.69e+07  &         0.000        &      6.4e+14    &      6.4e+14     \\\\\n",
       "\\textbf{x11}   &       4.6450  &     2.57e-08     &  1.81e+08  &         0.000        &        4.645    &        4.645     \\\\\n",
       "\\textbf{x12}   &      5.6e+14  &     3.79e+06     &  1.48e+08  &         0.000        &      5.6e+14    &      5.6e+14     \\\\\n",
       "\\textbf{x13}   &    2.723e+14  &     3.66e+06     &  7.43e+07  &         0.000        &     2.72e+14    &     2.72e+14     \\\\\n",
       "\\textbf{x14}   &   -1.533e+14  &     3.46e+06     & -4.43e+07  &         0.000        &    -1.53e+14    &    -1.53e+14     \\\\\n",
       "\\textbf{x15}   &      -0.6957  &     3.93e-08     & -1.77e+07  &         0.000        &       -0.696    &       -0.696     \\\\\n",
       "\\textbf{x16}   &    1.005e+15  &     3.46e+06     &  2.91e+08  &         0.000        &     1.01e+15    &     1.01e+15     \\\\\n",
       "\\textbf{x17}   &    4.461e+15  &     5.37e+06     &  8.31e+08  &         0.000        &     4.46e+15    &     4.46e+15     \\\\\n",
       "\\textbf{x18}   &     3.12e+13  &      8.4e+06     &  3.71e+06  &         0.000        &     3.12e+13    &     3.12e+13     \\\\\n",
       "\\textbf{x19}   &    -5.98e+14  &     1.14e+07     & -5.22e+07  &         0.000        &    -5.98e+14    &    -5.98e+14     \\\\\n",
       "\\textbf{x20}   &   -6.433e+14  &     1.91e+07     & -3.38e+07  &         0.000        &    -6.43e+14    &    -6.43e+14     \\\\\n",
       "\\textbf{x21}   &    4.152e+14  &     2.09e+07     &  1.99e+07  &         0.000        &     4.15e+14    &     4.15e+14     \\\\\n",
       "\\textbf{x22}   &    6.722e+14  &     1.74e+07     &  3.86e+07  &         0.000        &     6.72e+14    &     6.72e+14     \\\\\n",
       "\\textbf{x23}   &    2.039e+14  &     8.47e+06     &  2.41e+07  &         0.000        &     2.04e+14    &     2.04e+14     \\\\\n",
       "\\textbf{x24}   &   -3.282e+14  &     3.86e+06     &  -8.5e+07  &         0.000        &    -3.28e+14    &    -3.28e+14     \\\\\n",
       "\\textbf{x25}   &    8.484e+15  &     3.25e+06     &  2.61e+09  &         0.000        &     8.48e+15    &     8.48e+15     \\\\\n",
       "\\textbf{x26}   &    4.264e+14  &     3.45e+06     &  1.23e+08  &         0.000        &     4.26e+14    &     4.26e+14     \\\\\n",
       "\\textbf{x27}   &   -7.088e+14  &     4.37e+06     & -1.62e+08  &         0.000        &    -7.09e+14    &    -7.09e+14     \\\\\n",
       "\\textbf{x28}   &    2.137e+14  &     3.59e+06     &  5.95e+07  &         0.000        &     2.14e+14    &     2.14e+14     \\\\\n",
       "\\textbf{x29}   &    2.402e+14  &     3.79e+06     &  6.33e+07  &         0.000        &      2.4e+14    &      2.4e+14     \\\\\n",
       "\\textbf{x30}   &   -9.225e+14  &     4.93e+06     & -1.87e+08  &         0.000        &    -9.22e+14    &    -9.22e+14     \\\\\n",
       "\\textbf{x31}   &   -5.008e+14  &     5.44e+06     &  -9.2e+07  &         0.000        &    -5.01e+14    &    -5.01e+14     \\\\\n",
       "\\textbf{x32}   &    6.854e+14  &     4.77e+06     &  1.44e+08  &         0.000        &     6.85e+14    &     6.85e+14     \\\\\n",
       "\\textbf{x33}   &    9.775e+14  &     7.36e+06     &  1.33e+08  &         0.000        &     9.78e+14    &     9.78e+14     \\\\\n",
       "\\textbf{x34}   &    1.558e+15  &     1.33e+07     &  1.17e+08  &         0.000        &     1.56e+15    &     1.56e+15     \\\\\n",
       "\\textbf{x35}   &    1.137e+15  &     1.15e+07     &  9.92e+07  &         0.000        &     1.14e+15    &     1.14e+15     \\\\\n",
       "\\textbf{x36}   &    6.038e+14  &     6.27e+06     &  9.63e+07  &         0.000        &     6.04e+14    &     6.04e+14     \\\\\n",
       "\\textbf{x37}   &    1.131e+15  &     5.67e+06     &  1.99e+08  &         0.000        &     1.13e+15    &     1.13e+15     \\\\\n",
       "\\textbf{x38}   &    2.161e+14  &     4.94e+06     &  4.37e+07  &         0.000        &     2.16e+14    &     2.16e+14     \\\\\n",
       "\\textbf{x39}   &   -2.376e+14  &     1.99e+07     & -1.19e+07  &         0.000        &    -2.38e+14    &    -2.38e+14     \\\\\n",
       "\\textbf{x40}   &    5.929e+12  &     3.59e+07     &  1.65e+05  &         0.000        &     5.93e+12    &     5.93e+12     \\\\\n",
       "\\textbf{x41}   &   -3.116e+14  &     3.75e+07     &  -8.3e+06  &         0.000        &    -3.12e+14    &    -3.12e+14     \\\\\n",
       "\\textbf{x42}   &    2.704e+14  &     3.65e+06     &   7.4e+07  &         0.000        &      2.7e+14    &      2.7e+14     \\\\\n",
       "\\textbf{x43}   &    8.093e+14  &     3.77e+06     &  2.15e+08  &         0.000        &     8.09e+14    &     8.09e+14     \\\\\n",
       "\\textbf{x44}   &    7.558e+14  &     3.49e+06     &  2.17e+08  &         0.000        &     7.56e+14    &     7.56e+14     \\\\\n",
       "\\textbf{x45}   &   -3.222e+14  &     1.91e+06     & -1.69e+08  &         0.000        &    -3.22e+14    &    -3.22e+14     \\\\\n",
       "\\textbf{x46}   &    3.222e+14  &     1.91e+06     &  1.69e+08  &         0.000        &     3.22e+14    &     3.22e+14     \\\\\n",
       "\\textbf{x47}   &   -2.411e+14  &      4.1e+06     & -5.88e+07  &         0.000        &    -2.41e+14    &    -2.41e+14     \\\\\n",
       "\\textbf{x48}   &    5.629e+14  &     4.67e+06     &  1.21e+08  &         0.000        &     5.63e+14    &     5.63e+14     \\\\\n",
       "\\textbf{x49}   &    2.481e+14  &     5.97e+06     &  4.15e+07  &         0.000        &     2.48e+14    &     2.48e+14     \\\\\n",
       "\\textbf{x50}   &    3.703e+14  &     7.49e+06     &  4.94e+07  &         0.000        &      3.7e+14    &      3.7e+14     \\\\\n",
       "\\textbf{x51}   &    8.253e+14  &     7.04e+06     &  1.17e+08  &         0.000        &     8.25e+14    &     8.25e+14     \\\\\n",
       "\\textbf{x52}   &    6.558e+14  &     7.16e+06     &  9.16e+07  &         0.000        &     6.56e+14    &     6.56e+14     \\\\\n",
       "\\textbf{x53}   &     9.72e+14  &     6.24e+06     &  1.56e+08  &         0.000        &     9.72e+14    &     9.72e+14     \\\\\n",
       "\\textbf{x54}   &    1.118e+15  &     7.22e+06     &  1.55e+08  &         0.000        &     1.12e+15    &     1.12e+15     \\\\\n",
       "\\textbf{x55}   &    2.039e+13  &     5.98e+06     &  3.41e+06  &         0.000        &     2.04e+13    &     2.04e+13     \\\\\n",
       "\\textbf{x56}   &   -5.894e+13  &     5.42e+06     & -1.09e+07  &         0.000        &    -5.89e+13    &    -5.89e+13     \\\\\n",
       "\\textbf{x57}   &    5.884e+14  &     5.67e+06     &  1.04e+08  &         0.000        &     5.88e+14    &     5.88e+14     \\\\\n",
       "\\textbf{x58}   &    -1.79e+14  &     3.79e+06     & -4.72e+07  &         0.000        &    -1.79e+14    &    -1.79e+14     \\\\\n",
       "\\textbf{x59}   &    2.823e+13  &     1.67e+06     &  1.69e+07  &         0.000        &     2.82e+13    &     2.82e+13     \\\\\n",
       "\\textbf{x60}   &    3.892e+11  &     3.97e+06     &  9.79e+04  &         0.000        &     3.89e+11    &     3.89e+11     \\\\\n",
       "\\textbf{x61}   &   -3.674e+14  &     4.04e+06     & -9.09e+07  &         0.000        &    -3.67e+14    &    -3.67e+14     \\\\\n",
       "\\textbf{x62}   &   -9.883e+14  &     3.48e+06     & -2.84e+08  &         0.000        &    -9.88e+14    &    -9.88e+14     \\\\\n",
       "\\textbf{x63}   &   -7.009e+14  &     3.49e+06     & -2.01e+08  &         0.000        &    -7.01e+14    &    -7.01e+14     \\\\\n",
       "\\textbf{x64}   &    9.966e+14  &     3.45e+06     &  2.89e+08  &         0.000        &     9.97e+14    &     9.97e+14     \\\\\n",
       "\\textbf{x65}   &    -5.65e+14  &     3.27e+06     & -1.73e+08  &         0.000        &    -5.65e+14    &    -5.65e+14     \\\\\n",
       "\\textbf{x66}   &    3.655e+15  &      3.2e+06     &  1.14e+09  &         0.000        &     3.65e+15    &     3.65e+15     \\\\\n",
       "\\textbf{x67}   &   -3.421e+14  &     3.05e+06     & -1.12e+08  &         0.000        &    -3.42e+14    &    -3.42e+14     \\\\\n",
       "\\textbf{x68}   &    -9.28e+14  &     3.32e+06     &  -2.8e+08  &         0.000        &    -9.28e+14    &    -9.28e+14     \\\\\n",
       "\\textbf{x69}   &   -9.125e+14  &      2.7e+06     & -3.37e+08  &         0.000        &    -9.13e+14    &    -9.13e+14     \\\\\n",
       "\\textbf{x70}   &   -3.657e+14  &     1.94e+06     & -1.89e+08  &         0.000        &    -3.66e+14    &    -3.66e+14     \\\\\n",
       "\\textbf{x71}   &    2.823e+13  &     1.67e+06     &  1.69e+07  &         0.000        &     2.82e+13    &     2.82e+13     \\\\\n",
       "\\textbf{x72}   &   -2.823e+13  &     1.67e+06     & -1.69e+07  &         0.000        &    -2.82e+13    &    -2.82e+13     \\\\\n",
       "\\textbf{x73}   &    1.078e+14  &     3.53e+06     &  3.06e+07  &         0.000        &     1.08e+14    &     1.08e+14     \\\\\n",
       "\\textbf{x74}   &   -1.426e+15  &     3.91e+06     & -3.65e+08  &         0.000        &    -1.43e+15    &    -1.43e+15     \\\\\n",
       "\\textbf{x75}   &    2.345e+13  &     3.46e+06     &  6.79e+06  &         0.000        &     2.34e+13    &     2.34e+13     \\\\\n",
       "\\textbf{x76}   &   -4.738e+14  &     4.35e+06     & -1.09e+08  &         0.000        &    -4.74e+14    &    -4.74e+14     \\\\\n",
       "\\textbf{x77}   &    3.031e+14  &     4.35e+06     &  6.97e+07  &         0.000        &     3.03e+14    &     3.03e+14     \\\\\n",
       "\\textbf{x78}   &   -3.452e+14  &     5.21e+06     & -6.62e+07  &         0.000        &    -3.45e+14    &    -3.45e+14     \\\\\n",
       "\\textbf{x79}   &   -4.826e+14  &     4.77e+06     & -1.01e+08  &         0.000        &    -4.83e+14    &    -4.83e+14     \\\\\n",
       "\\textbf{x80}   &   -3.359e+14  &     6.74e+06     & -4.98e+07  &         0.000        &    -3.36e+14    &    -3.36e+14     \\\\\n",
       "\\textbf{x81}   &    2.364e+14  &        5e+06     &  4.73e+07  &         0.000        &     2.36e+14    &     2.36e+14     \\\\\n",
       "\\textbf{x82}   &   -1.282e+15  &     5.08e+06     & -2.52e+08  &         0.000        &    -1.28e+15    &    -1.28e+15     \\\\\n",
       "\\textbf{x83}   &    -2.89e+14  &     4.32e+06     &  -6.7e+07  &         0.000        &    -2.89e+14    &    -2.89e+14     \\\\\n",
       "\\textbf{x84}   &   -1.181e+15  &     4.29e+06     & -2.75e+08  &         0.000        &    -1.18e+15    &    -1.18e+15     \\\\\n",
       "\\textbf{x85}   &    4.828e+13  &     4.57e+06     &  1.06e+07  &         0.000        &     4.83e+13    &     4.83e+13     \\\\\n",
       "\\textbf{x86}   &    -4.65e+15  &     3.61e+06     & -1.29e+09  &         0.000        &    -4.65e+15    &    -4.65e+15     \\\\\n",
       "\\textbf{x87}   &   -2.322e+14  &     3.58e+06     & -6.49e+07  &         0.000        &    -2.32e+14    &    -2.32e+14     \\\\\n",
       "\\textbf{x88}   &   -1.826e+16  &     3.39e+06     & -5.38e+09  &         0.000        &    -1.83e+16    &    -1.83e+16     \\\\\n",
       "\\textbf{x89}   &   -5.126e+15  &     3.51e+06     & -1.46e+09  &         0.000        &    -5.13e+15    &    -5.13e+15     \\\\\n",
       "\\textbf{x90}   &     1.59e+15  &     4.44e+06     &  3.58e+08  &         0.000        &     1.59e+15    &     1.59e+15     \\\\\n",
       "\\textbf{x91}   &   -1.226e+15  &     3.27e+06     & -3.75e+08  &         0.000        &    -1.23e+15    &    -1.23e+15     \\\\\n",
       "\\textbf{x92}   &   -7.676e+14  &     3.74e+06     & -2.05e+08  &         0.000        &    -7.68e+14    &    -7.68e+14     \\\\\n",
       "\\textbf{x93}   &    4.021e+12  &     4.02e+06     &     1e+06  &         0.000        &     4.02e+12    &     4.02e+12     \\\\\n",
       "\\textbf{x94}   &   -4.669e+14  &      4.1e+06     & -1.14e+08  &         0.000        &    -4.67e+14    &    -4.67e+14     \\\\\n",
       "\\textbf{x95}   &    1.246e+13  &     4.28e+06     &  2.91e+06  &         0.000        &     1.25e+13    &     1.25e+13     \\\\\n",
       "\\textbf{x96}   &   -1.033e+15  &     4.13e+06     &  -2.5e+08  &         0.000        &    -1.03e+15    &    -1.03e+15     \\\\\n",
       "\\textbf{x97}   &   -1.203e+15  &     3.99e+06     & -3.02e+08  &         0.000        &     -1.2e+15    &     -1.2e+15     \\\\\n",
       "\\textbf{x98}   &   -1.589e+15  &     4.64e+06     & -3.43e+08  &         0.000        &    -1.59e+15    &    -1.59e+15     \\\\\n",
       "\\textbf{x99}   &   -4.923e+14  &     6.55e+06     & -7.52e+07  &         0.000        &    -4.92e+14    &    -4.92e+14     \\\\\n",
       "\\textbf{x100}  &    1.028e+14  &     6.57e+06     &  1.57e+07  &         0.000        &     1.03e+14    &     1.03e+14     \\\\\n",
       "\\textbf{x101}  &   -1.276e+14  &     6.39e+06     &    -2e+07  &         0.000        &    -1.28e+14    &    -1.28e+14     \\\\\n",
       "\\textbf{x102}  &    2.421e+15  &     3.43e+06     &  7.06e+08  &         0.000        &     2.42e+15    &     2.42e+15     \\\\\n",
       "\\textbf{x103}  &   -1.418e+14  &     4.86e+06     & -2.92e+07  &         0.000        &    -1.42e+14    &    -1.42e+14     \\\\\n",
       "\\textbf{x104}  &   -1.043e+15  &     4.79e+06     & -2.18e+08  &         0.000        &    -1.04e+15    &    -1.04e+15     \\\\\n",
       "\\textbf{x105}  &    1.143e+15  &     4.17e+06     &  2.74e+08  &         0.000        &     1.14e+15    &     1.14e+15     \\\\\n",
       "\\textbf{x106}  &    -3.73e+13  &     3.68e+06     & -1.01e+07  &         0.000        &    -3.73e+13    &    -3.73e+13     \\\\\n",
       "\\textbf{x107}  &   -1.901e+14  &     3.72e+06     & -5.12e+07  &         0.000        &     -1.9e+14    &     -1.9e+14     \\\\\n",
       "\\textbf{x108}  &   -1.168e+15  &     3.55e+06     &  -3.3e+08  &         0.000        &    -1.17e+15    &    -1.17e+15     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{Generalized Linear Model Regression Results}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:           m_gq_dropout   No. Observations:                  498\n",
       "Model:                            GLM   Df Residuals:                      396\n",
       "Model Family:                Binomial   Df Model:                          101\n",
       "Link Function:                  Logit   Scale:                          1.0000\n",
       "Method:                          IRLS   Log-Likelihood:                    nan\n",
       "Date:                Fri, 14 Nov 2025   Deviance:                       15289.\n",
       "Time:                        17:52:25   Pearson chi2:                 7.48e+17\n",
       "No. Iterations:                    79   Pseudo R-squ. (CS):                nan\n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1          3.894e+14   3.59e+06   1.09e+08      0.000    3.89e+14    3.89e+14\n",
       "x2          1.426e+14   3.61e+06   3.95e+07      0.000    1.43e+14    1.43e+14\n",
       "x3          1.504e+14   3.58e+06    4.2e+07      0.000     1.5e+14     1.5e+14\n",
       "x4         -9.073e+13   3.52e+06  -2.58e+07      0.000   -9.07e+13   -9.07e+13\n",
       "x5          9.862e+14   4.09e+06   2.41e+08      0.000    9.86e+14    9.86e+14\n",
       "x6          3.603e+14   4.34e+06   8.29e+07      0.000     3.6e+14     3.6e+14\n",
       "x7         -1.437e+14   4.61e+06  -3.12e+07      0.000   -1.44e+14   -1.44e+14\n",
       "const          1.5487   2.31e-08    6.7e+07      0.000       1.549       1.549\n",
       "x8         -7.347e+14   1.39e+07  -5.28e+07      0.000   -7.35e+14   -7.35e+14\n",
       "x9          1.055e+15   1.87e+07   5.63e+07      0.000    1.06e+15    1.06e+15\n",
       "x10         6.401e+14   1.73e+07   3.69e+07      0.000     6.4e+14     6.4e+14\n",
       "x11            4.6450   2.57e-08   1.81e+08      0.000       4.645       4.645\n",
       "x12           5.6e+14   3.79e+06   1.48e+08      0.000     5.6e+14     5.6e+14\n",
       "x13         2.723e+14   3.66e+06   7.43e+07      0.000    2.72e+14    2.72e+14\n",
       "x14        -1.533e+14   3.46e+06  -4.43e+07      0.000   -1.53e+14   -1.53e+14\n",
       "x15           -0.6957   3.93e-08  -1.77e+07      0.000      -0.696      -0.696\n",
       "x16         1.005e+15   3.46e+06   2.91e+08      0.000    1.01e+15    1.01e+15\n",
       "x17         4.461e+15   5.37e+06   8.31e+08      0.000    4.46e+15    4.46e+15\n",
       "x18          3.12e+13    8.4e+06   3.71e+06      0.000    3.12e+13    3.12e+13\n",
       "x19         -5.98e+14   1.14e+07  -5.22e+07      0.000   -5.98e+14   -5.98e+14\n",
       "x20        -6.433e+14   1.91e+07  -3.38e+07      0.000   -6.43e+14   -6.43e+14\n",
       "x21         4.152e+14   2.09e+07   1.99e+07      0.000    4.15e+14    4.15e+14\n",
       "x22         6.722e+14   1.74e+07   3.86e+07      0.000    6.72e+14    6.72e+14\n",
       "x23         2.039e+14   8.47e+06   2.41e+07      0.000    2.04e+14    2.04e+14\n",
       "x24        -3.282e+14   3.86e+06   -8.5e+07      0.000   -3.28e+14   -3.28e+14\n",
       "x25         8.484e+15   3.25e+06   2.61e+09      0.000    8.48e+15    8.48e+15\n",
       "x26         4.264e+14   3.45e+06   1.23e+08      0.000    4.26e+14    4.26e+14\n",
       "x27        -7.088e+14   4.37e+06  -1.62e+08      0.000   -7.09e+14   -7.09e+14\n",
       "x28         2.137e+14   3.59e+06   5.95e+07      0.000    2.14e+14    2.14e+14\n",
       "x29         2.402e+14   3.79e+06   6.33e+07      0.000     2.4e+14     2.4e+14\n",
       "x30        -9.225e+14   4.93e+06  -1.87e+08      0.000   -9.22e+14   -9.22e+14\n",
       "x31        -5.008e+14   5.44e+06   -9.2e+07      0.000   -5.01e+14   -5.01e+14\n",
       "x32         6.854e+14   4.77e+06   1.44e+08      0.000    6.85e+14    6.85e+14\n",
       "x33         9.775e+14   7.36e+06   1.33e+08      0.000    9.78e+14    9.78e+14\n",
       "x34         1.558e+15   1.33e+07   1.17e+08      0.000    1.56e+15    1.56e+15\n",
       "x35         1.137e+15   1.15e+07   9.92e+07      0.000    1.14e+15    1.14e+15\n",
       "x36         6.038e+14   6.27e+06   9.63e+07      0.000    6.04e+14    6.04e+14\n",
       "x37         1.131e+15   5.67e+06   1.99e+08      0.000    1.13e+15    1.13e+15\n",
       "x38         2.161e+14   4.94e+06   4.37e+07      0.000    2.16e+14    2.16e+14\n",
       "x39        -2.376e+14   1.99e+07  -1.19e+07      0.000   -2.38e+14   -2.38e+14\n",
       "x40         5.929e+12   3.59e+07   1.65e+05      0.000    5.93e+12    5.93e+12\n",
       "x41        -3.116e+14   3.75e+07   -8.3e+06      0.000   -3.12e+14   -3.12e+14\n",
       "x42         2.704e+14   3.65e+06    7.4e+07      0.000     2.7e+14     2.7e+14\n",
       "x43         8.093e+14   3.77e+06   2.15e+08      0.000    8.09e+14    8.09e+14\n",
       "x44         7.558e+14   3.49e+06   2.17e+08      0.000    7.56e+14    7.56e+14\n",
       "x45        -3.222e+14   1.91e+06  -1.69e+08      0.000   -3.22e+14   -3.22e+14\n",
       "x46         3.222e+14   1.91e+06   1.69e+08      0.000    3.22e+14    3.22e+14\n",
       "x47        -2.411e+14    4.1e+06  -5.88e+07      0.000   -2.41e+14   -2.41e+14\n",
       "x48         5.629e+14   4.67e+06   1.21e+08      0.000    5.63e+14    5.63e+14\n",
       "x49         2.481e+14   5.97e+06   4.15e+07      0.000    2.48e+14    2.48e+14\n",
       "x50         3.703e+14   7.49e+06   4.94e+07      0.000     3.7e+14     3.7e+14\n",
       "x51         8.253e+14   7.04e+06   1.17e+08      0.000    8.25e+14    8.25e+14\n",
       "x52         6.558e+14   7.16e+06   9.16e+07      0.000    6.56e+14    6.56e+14\n",
       "x53          9.72e+14   6.24e+06   1.56e+08      0.000    9.72e+14    9.72e+14\n",
       "x54         1.118e+15   7.22e+06   1.55e+08      0.000    1.12e+15    1.12e+15\n",
       "x55         2.039e+13   5.98e+06   3.41e+06      0.000    2.04e+13    2.04e+13\n",
       "x56        -5.894e+13   5.42e+06  -1.09e+07      0.000   -5.89e+13   -5.89e+13\n",
       "x57         5.884e+14   5.67e+06   1.04e+08      0.000    5.88e+14    5.88e+14\n",
       "x58         -1.79e+14   3.79e+06  -4.72e+07      0.000   -1.79e+14   -1.79e+14\n",
       "x59         2.823e+13   1.67e+06   1.69e+07      0.000    2.82e+13    2.82e+13\n",
       "x60         3.892e+11   3.97e+06   9.79e+04      0.000    3.89e+11    3.89e+11\n",
       "x61        -3.674e+14   4.04e+06  -9.09e+07      0.000   -3.67e+14   -3.67e+14\n",
       "x62        -9.883e+14   3.48e+06  -2.84e+08      0.000   -9.88e+14   -9.88e+14\n",
       "x63        -7.009e+14   3.49e+06  -2.01e+08      0.000   -7.01e+14   -7.01e+14\n",
       "x64         9.966e+14   3.45e+06   2.89e+08      0.000    9.97e+14    9.97e+14\n",
       "x65         -5.65e+14   3.27e+06  -1.73e+08      0.000   -5.65e+14   -5.65e+14\n",
       "x66         3.655e+15    3.2e+06   1.14e+09      0.000    3.65e+15    3.65e+15\n",
       "x67        -3.421e+14   3.05e+06  -1.12e+08      0.000   -3.42e+14   -3.42e+14\n",
       "x68         -9.28e+14   3.32e+06   -2.8e+08      0.000   -9.28e+14   -9.28e+14\n",
       "x69        -9.125e+14    2.7e+06  -3.37e+08      0.000   -9.13e+14   -9.13e+14\n",
       "x70        -3.657e+14   1.94e+06  -1.89e+08      0.000   -3.66e+14   -3.66e+14\n",
       "x71         2.823e+13   1.67e+06   1.69e+07      0.000    2.82e+13    2.82e+13\n",
       "x72        -2.823e+13   1.67e+06  -1.69e+07      0.000   -2.82e+13   -2.82e+13\n",
       "x73         1.078e+14   3.53e+06   3.06e+07      0.000    1.08e+14    1.08e+14\n",
       "x74        -1.426e+15   3.91e+06  -3.65e+08      0.000   -1.43e+15   -1.43e+15\n",
       "x75         2.345e+13   3.46e+06   6.79e+06      0.000    2.34e+13    2.34e+13\n",
       "x76        -4.738e+14   4.35e+06  -1.09e+08      0.000   -4.74e+14   -4.74e+14\n",
       "x77         3.031e+14   4.35e+06   6.97e+07      0.000    3.03e+14    3.03e+14\n",
       "x78        -3.452e+14   5.21e+06  -6.62e+07      0.000   -3.45e+14   -3.45e+14\n",
       "x79        -4.826e+14   4.77e+06  -1.01e+08      0.000   -4.83e+14   -4.83e+14\n",
       "x80        -3.359e+14   6.74e+06  -4.98e+07      0.000   -3.36e+14   -3.36e+14\n",
       "x81         2.364e+14      5e+06   4.73e+07      0.000    2.36e+14    2.36e+14\n",
       "x82        -1.282e+15   5.08e+06  -2.52e+08      0.000   -1.28e+15   -1.28e+15\n",
       "x83         -2.89e+14   4.32e+06   -6.7e+07      0.000   -2.89e+14   -2.89e+14\n",
       "x84        -1.181e+15   4.29e+06  -2.75e+08      0.000   -1.18e+15   -1.18e+15\n",
       "x85         4.828e+13   4.57e+06   1.06e+07      0.000    4.83e+13    4.83e+13\n",
       "x86         -4.65e+15   3.61e+06  -1.29e+09      0.000   -4.65e+15   -4.65e+15\n",
       "x87        -2.322e+14   3.58e+06  -6.49e+07      0.000   -2.32e+14   -2.32e+14\n",
       "x88        -1.826e+16   3.39e+06  -5.38e+09      0.000   -1.83e+16   -1.83e+16\n",
       "x89        -5.126e+15   3.51e+06  -1.46e+09      0.000   -5.13e+15   -5.13e+15\n",
       "x90          1.59e+15   4.44e+06   3.58e+08      0.000    1.59e+15    1.59e+15\n",
       "x91        -1.226e+15   3.27e+06  -3.75e+08      0.000   -1.23e+15   -1.23e+15\n",
       "x92        -7.676e+14   3.74e+06  -2.05e+08      0.000   -7.68e+14   -7.68e+14\n",
       "x93         4.021e+12   4.02e+06      1e+06      0.000    4.02e+12    4.02e+12\n",
       "x94        -4.669e+14    4.1e+06  -1.14e+08      0.000   -4.67e+14   -4.67e+14\n",
       "x95         1.246e+13   4.28e+06   2.91e+06      0.000    1.25e+13    1.25e+13\n",
       "x96        -1.033e+15   4.13e+06   -2.5e+08      0.000   -1.03e+15   -1.03e+15\n",
       "x97        -1.203e+15   3.99e+06  -3.02e+08      0.000    -1.2e+15    -1.2e+15\n",
       "x98        -1.589e+15   4.64e+06  -3.43e+08      0.000   -1.59e+15   -1.59e+15\n",
       "x99        -4.923e+14   6.55e+06  -7.52e+07      0.000   -4.92e+14   -4.92e+14\n",
       "x100        1.028e+14   6.57e+06   1.57e+07      0.000    1.03e+14    1.03e+14\n",
       "x101       -1.276e+14   6.39e+06     -2e+07      0.000   -1.28e+14   -1.28e+14\n",
       "x102        2.421e+15   3.43e+06   7.06e+08      0.000    2.42e+15    2.42e+15\n",
       "x103       -1.418e+14   4.86e+06  -2.92e+07      0.000   -1.42e+14   -1.42e+14\n",
       "x104       -1.043e+15   4.79e+06  -2.18e+08      0.000   -1.04e+15   -1.04e+15\n",
       "x105        1.143e+15   4.17e+06   2.74e+08      0.000    1.14e+15    1.14e+15\n",
       "x106        -3.73e+13   3.68e+06  -1.01e+07      0.000   -3.73e+13   -3.73e+13\n",
       "x107       -1.901e+14   3.72e+06  -5.12e+07      0.000    -1.9e+14    -1.9e+14\n",
       "x108       -1.168e+15   3.55e+06   -3.3e+08      0.000   -1.17e+15   -1.17e+15\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Standardize and add constants to X matrices\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_train_std = sm.add_constant(X_train_scaled) \n",
    "X_test_std = sm.add_constant(X_test_scaled)\n",
    "\n",
    "#Fit logistic model on training data\n",
    "glm = sm.GLM(Y_train,\n",
    "             X_train_scaled,\n",
    "             family=sm.families.Binomial())\n",
    "results_logit = glm.fit()\n",
    "results_logit.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af428b6f-e49f-4b24-ac07-ab47e0ee6f2e",
   "metadata": {},
   "source": [
    "**3. Obtain the predicted values of the response variable and use them to compute the accuracy rate of your model. Compare it to the accuracy rate you would obtain if you predicted the endline enrollment status by flipping a coin. What is the percentage of improvement in accuracy that you achieve?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dc20945-cf1a-4662-9c6c-35871b1ad108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5602409638554218\n",
      "Percentage of improvement: 12.048192771084354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/python/lib/python3.13/site-packages/statsmodels/genmod/families/links.py:198: RuntimeWarning: overflow encountered in exp\n",
      "  t = np.exp(-z)\n"
     ]
    }
   ],
   "source": [
    "#Compute predicted probabilities\n",
    "probs_logit = results_logit.predict(X_test_std)\n",
    "\n",
    "#Transform probabilities into predicted failure/success\n",
    "Y_pred_logit = np.array([0]*X_test_std.shape[0])\n",
    "Y_pred_logit[probs_logit>0.5]= 1\n",
    "\n",
    "#Compute error and accuracy rates\n",
    "error_rate_logit = np.mean(Y_pred_logit != Y_test)\n",
    "accuracy_logit = 1 - error_rate_logit\n",
    "print(accuracy_logit)\n",
    "\n",
    "#Compute percentage of improvement\n",
    "imp = (accuracy_logit - 0.5) / 0.5 * 100\n",
    "print(\"Percentage of improvement:\", imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a64cb5-af4c-44e9-ad2b-574114c8b38b",
   "metadata": {},
   "source": [
    "**4. Fit a LASSO model to the same data, using the same predictors. Use 5-fold cross-validation to select the optimal lambda and a validation set to obtain a test MSE. Always use a seed of 3.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a71f56f3-19a6-4f3f-a75a-b67617775963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Y and X\n",
    "Y = df['m_gq_dropout']\n",
    "X = df[features]\n",
    "\n",
    "# Create search grid for tuning parameter (lambda)\n",
    "lambdas = 10**np.linspace(8, -2, 100) / Y.std()\n",
    "\n",
    "# Choose splitting rule for outer split (train set vs. test set)\n",
    "outer_valid = skm.ShuffleSplit(n_splits=1, \n",
    "                               test_size=0.25,\n",
    "                               random_state=3)\n",
    "\n",
    "# Choose splitting rule for cross-validation step (used to select optimal lambda)\n",
    "inner_cv = skm.KFold(n_splits=5,\n",
    "                     shuffle=True,\n",
    "                     random_state=3)\n",
    "\n",
    "# Program lasso regression with lambda selected based on inner_CV\n",
    "lassoCV = skl.ElasticNetCV(alphas=lambdas,\n",
    "                           l1_ratio=1,\n",
    "                           cv=inner_cv)\n",
    "\n",
    "# Standardize predictors: \n",
    "scaler = StandardScaler(with_mean=True,  with_std=True)\n",
    "pipeCV = Pipeline(steps=[('scaler', scaler),\n",
    "                         ('lasso', lassoCV)])\n",
    "\n",
    "# Obtain fitted model from validation set using outer split\n",
    "results_lasso = skm.cross_validate(pipeCV, \n",
    "                             X,\n",
    "                             Y,\n",
    "                             cv=outer_valid,\n",
    "                             scoring='neg_mean_squared_error',\n",
    "                             return_estimator=True) #Add this line to store estimator results (to print selected alpha below)\n",
    "\n",
    "est = results_lasso['estimator'][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cff266-cf9d-4682-b118-e39d77485790",
   "metadata": {},
   "source": [
    "**5. Obtain the predicted values of the response variable and use them to compute the accuracy rate of your LASSO model. Compare it to the accuracy rate of your logit model. What is the percentage of improvement in accuracy that you achieve?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2152c29-8f45-4e55-8447-bbd082fb30d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.608433734939759\n",
      "Percentage of improvement over coin: 21.68674698795181\n",
      "Percentage of improvement over logit: 8.602150537634396\n"
     ]
    }
   ],
   "source": [
    "#Obtain matrix of predictor values for the test sample in the outer split\n",
    "for train_idx, test_idx in outer_valid.split(X):\n",
    "\n",
    "    X_train_lasso, X_test_lasso = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    Y_train_lasso, Y_test_lasso = Y.iloc[train_idx], Y.iloc[test_idx]\n",
    "\n",
    "#Compute success probabilities and convert them to dummy\n",
    "prob_lasso = est.predict(X_test_lasso)\n",
    "Y_pred_lasso = np.array([0]*X_test_lasso.shape[0])\n",
    "Y_pred_lasso[prob_lasso>0.5]= 1\n",
    "\n",
    "#Compute accuracy rate of LASSO\n",
    "error_rate_lasso = np.mean(Y_pred_lasso != Y_test_lasso)\n",
    "accuracy_lasso = 1 - error_rate_lasso\n",
    "print(accuracy_lasso)\n",
    "np.mean(Y_test)\n",
    "\n",
    "#Compute percentage of improvement over coin\n",
    "imp_coin = (accuracy_lasso - 0.5) / 0.5 * 100\n",
    "print(\"Percentage of improvement over coin:\", imp_coin)\n",
    "\n",
    "#Compute percentage of improvement over logit\n",
    "imp_logit = (accuracy_lasso - accuracy_logit) / accuracy_logit * 100\n",
    "print(\"Percentage of improvement over logit:\", imp_logit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351668f8-1abe-4307-a928-4d955026b261",
   "metadata": {},
   "source": [
    "**6. Create a dataframe containing one column for the labels of predictor variables and another one with the coefficients on each predictor variable in the trained LASSO model. What is the predictor for which the coefficient has the largest absolute value? Hint : you can use DataFrame.sort_values(by = '', ascending=False) to sort your dataframe (replace DataFrame by the name of your dataframe and pass the name of the column on which you want to sort to the by argument)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c0c4a8c-62e9-40cc-b776-f0ffa0a036dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefs</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.009134</td>\n",
       "      <td>b_gq_g_Index_auto_cat3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.002621</td>\n",
       "      <td>b_gq_c_40_all_16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000568</td>\n",
       "      <td>b_gq_c1_ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000095</td>\n",
       "      <td>b_gq_age17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>b_hq_a2_brick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>-0.010893</td>\n",
       "      <td>b_gq_c_40_all_24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.013641</td>\n",
       "      <td>b_hq_a2_stone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>-0.015289</td>\n",
       "      <td>b_hq_hh_b9_60_65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>-0.021764</td>\n",
       "      <td>b_hq_hh_b10_wido</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>-0.067766</td>\n",
       "      <td>b_hq_a6g_dum</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       coefs                  labels\n",
       "40  0.009134  b_gq_g_Index_auto_cat3\n",
       "76  0.002621        b_gq_c_40_all_16\n",
       "25  0.000568            b_gq_c1_ever\n",
       "17  0.000095              b_gq_age17\n",
       "2  -0.000000           b_hq_a2_brick\n",
       "..       ...                     ...\n",
       "84 -0.010893        b_gq_c_40_all_24\n",
       "1  -0.013641           b_hq_a2_stone\n",
       "55 -0.015289        b_hq_hh_b9_60_65\n",
       "61 -0.021764        b_hq_hh_b10_wido\n",
       "74 -0.067766            b_hq_a6g_dum\n",
       "\n",
       "[109 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print selected coefs\n",
    "selected_coefs = est.named_steps['lasso'].coef_\n",
    "coefs = pd.DataFrame({'coefs': selected_coefs, 'labels': features})\n",
    "coefs = coefs.sort_values(by='coefs', ascending=False)\n",
    "coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f3c263-2386-4a0f-8123-0285e06c19a0",
   "metadata": {},
   "source": [
    "The coefficient with the largest absolute magnitude is the coefficient on b_hq_a6g_dum (TV ownership dummy)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
