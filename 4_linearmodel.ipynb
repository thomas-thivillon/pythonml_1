{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3479aab9",
   "metadata": {},
   "source": [
    "# Lab: Shrinkage Methods\n",
    "\n",
    "This chapter follows closely chapter 6 of James et al. (2023)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709320fb-a381-4085-930c-c01a41eee827",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ISLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f32a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e643fba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install \"numpy<2.0\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a121caba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import subplots\n",
    "from statsmodels.api import OLS\n",
    "import sklearn.model_selection as skm\n",
    "import sklearn.linear_model as skl\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from ISLP import load_data\n",
    "from ISLP.models import ModelSpec as MS\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b846a10e",
   "metadata": {},
   "source": [
    "We again collect the new imports\n",
    "needed for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b5adfb",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from ISLP.models import \\\n",
    "     (Stepwise,\n",
    "      sklearn_selected,\n",
    "      sklearn_selection_path)\n",
    "#!pip install l0bnb\n",
    "#from l0bnb import fit_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d363de72",
   "metadata": {},
   "source": [
    "When we talk about big data, we do not only talk about larger sample size ($n$), but also about a larger number of dependant variables ($p$). However, with OLS, we are limited by the identification constraint that $p<n$. In addition, we would like to have $p$ way smaller than $n$ for inference and prediction accuracy.\n",
    "\n",
    "\n",
    "This lab presents methods to use a least squares fit in a setting in which the number of dependent variables,$p$, is large with respect to the sample size, $n$. \n",
    "\n",
    "\n",
    "## Shrinkage methods\n",
    "\n",
    "Model selection methods constrained the number of varaibles *before* running a linear regression. Shrinkage methods try to run a linear regression while constraining the number of covariates/predictor variables. In particular they penalize high values of the parameters in the objective function resulting in shrunken coefficients.\n",
    "\n",
    "We will use the `sklearn.linear_model` package (for which\n",
    "we use `skl` as shorthand below)\n",
    "to fit ridge and  lasso regularized linear models on the `Hitters` data.\n",
    "We start with the model matrix `X` (without an intercept) that we computed in the previous section on best subset regression.\n",
    " \n",
    "We will  apply shrinkage methods to the  `Hitters` \n",
    "data.  We wish to predict a baseball player’s `Salary` on the\n",
    "basis of various statistics associated with performance in the\n",
    "previous year.\n",
    "\n",
    "First of all, we note that the `Salary` variable is missing for\n",
    "some of the players.  The `np.isnan()`  function can be used to\n",
    "identify the missing observations. It returns an array\n",
    "of the same shape as the input vector, with a `True` for any elements that\n",
    "are missing, and a `False` for non-missing elements.  The\n",
    "`sum()`  method can then be used to count all of the\n",
    "missing elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b6e940",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Hitters = load_data('Hitters')\n",
    "np.isnan(Hitters['Salary']).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337d2256-2803-4bbe-8353-b063b669e6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hitters.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409d79c0",
   "metadata": {},
   "source": [
    " We see that `Salary` is missing for 59 players. The\n",
    "`dropna()`  method of data frames removes all of the rows that have missing\n",
    "values in any variable (by default --- see  `Hitters.dropna?`).\n",
    "Note: we are dropping the missing, but it is not always the best option. For instance in case of differential attrition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3472ea4b",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "Hitters = Hitters.dropna();\n",
    "Hitters.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a17487",
   "metadata": {},
   "source": [
    "We will fit the biggest model, using all the variables. Here we excluded the first column corresponding to the intercept, since the methods we will use will fit the intercept separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c796192",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "outputs": [],
   "source": [
    "design = MS(Hitters.columns.drop('Salary')).fit(Hitters)\n",
    "Y = np.array(Hitters['Salary'])\n",
    "D = design.fit_transform(Hitters)\n",
    "D = D.drop('intercept', axis=1)\n",
    "X = np.asarray(D)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2a9e92-ab9d-47db-ba16-28440e0b0365",
   "metadata": {},
   "source": [
    "## Shrinkage methods\n",
    "\n",
    "\n",
    "\n",
    "### Ridge Regression\n",
    "\n",
    "Recall that the Ridge Regression objective function is: \n",
    "$$ \\hat{\\beta}^{R}= arg min_{\\beta} \\sum_{i=1}^{n}(y_{i} - \\beta_{0} - \\sum_{j=1}^{p} \\beta_{j}x_{ij})^2 + \\lambda \\sum_{j=1}^p \\beta^{2}_{j}  = RSS + \\lambda \\sum_{j=1}^p \\beta^{2}_{j} $$\n",
    "with $\\lambda>0$ is a tuning parameter regulating the penalties applied on large parameter.\n",
    "\n",
    "\n",
    "We will use the function `skl.ElasticNet()` to fit both  ridge and the lasso.\n",
    "To fit a *path* of ridge regressions models, we use\n",
    "`skl.ElasticNet.path()`, which can fit both ridge and lasso, as well as a hybrid mixture;  ridge regression\n",
    "corresponds to `l1_ratio=0`.\n",
    "It is good practice to standardize the columns of `X` in these applications, why? The main reason is that if variables are measured in different units, those with higher values could be perceived as having higher contributions to the model while it is not the case. Since `skl.ElasticNet()` **does not do normalization**, we have to take care of that ourselves.\n",
    "\n",
    "Since we standardize first, in order to find coefficient\n",
    "estimates on the original scale, we must *unstandardize*\n",
    "the coefficient estimates. The parameter $\\lambda$ in $RSS + \\lambda \\sum^{p}_{j=1}\\beta^{2}_{j}$ (Ridge 6.5) and $RSS + \\lambda \\sum^{p}_{j=1}| \\beta_{j}| $ (Lasso 6.7) is called `alphas` in `sklearn`. In order to\n",
    "be consistent with the rest of this chapter, we use `lambdas`\n",
    "rather than `alphas` in what follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1fc533",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Standardize in the first 3 rows\n",
    "Xs = X - X.mean(0)[None,:]\n",
    "X_scale = X.std(0)\n",
    "Xs = Xs / X_scale[None,:]\n",
    "#Compute the lambdas\n",
    "lambdas = 10**np.linspace(8, -2, 100) / Y.std() #Creates a logarithmic grid between 10**8/sd(Y) et 10**−2/sd(Y).\n",
    "#Apply ridge regression\n",
    "soln_array = skl.ElasticNet.path(Xs,\n",
    "                                 Y,\n",
    "                                 l1_ratio=0.,\n",
    "                                 alphas=lambdas)[1]\n",
    "soln_array.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefe307b",
   "metadata": {},
   "source": [
    "Here we extract the array of coefficients corresponding to the solutions along the regularization path.\n",
    "\n",
    "By default the `skl.ElasticNet.path` method fits a path along\n",
    "an automatically selected range of $\\lambda$ values, except for the case when\n",
    "`l1_ratio=0`, which results in ridge regression (as is the case here). {The reason is rather technical; for all models except ridge, we can find the smallest value of $\\lambda$ for which all coefficients are zero. For ridge this value is $\\infty$.}  \n",
    "\n",
    "So here we have chosen to implement the function over a grid of values ranging\n",
    "from $\\lambda=10^{8}$ to $\\lambda=10^{-2}$ scaled by the standard\n",
    "deviation of $y$, essentially covering the full range of scenarios\n",
    "from the null model containing only the intercept, to the least\n",
    "squares fit.\n",
    "\n",
    "Associated with each value of $\\lambda$ is a vector of ridge\n",
    "regression coefficients,   that can be accessed by\n",
    "a column of `soln_array`. In this case, `soln_array` is a $19 \\times 100$ matrix, with\n",
    "19 rows (one for each predictor) and 100\n",
    "columns (one for each value of $\\lambda$).\n",
    "\n",
    "We transpose this matrix and turn it into a data frame to facilitate viewing and plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642287d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "soln_path = pd.DataFrame(soln_array.T,\n",
    "                         columns=D.columns,\n",
    "                         index=-np.log(lambdas)) #use neg log to have a readable and intuitive scale later in the graph.\n",
    "soln_path.index.name = 'negative log(lambda)' #positive values of lambda (higher penalization) have negative values of negative log lambda and conversely. \n",
    "soln_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2a5cd7",
   "metadata": {},
   "source": [
    "We plot the paths to get a sense of how the coefficients vary with $\\lambda$.\n",
    "To control the location of the legend we first set `legend` to `False` in the\n",
    "plot method, adding it afterward with the `legend()` method of `ax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734c1a05",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_fig, ax = subplots(figsize=(8,8))\n",
    "soln_path.plot(ax=ax, legend=False)\n",
    "ax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\n",
    "ax.set_ylabel('Standardized coefficients', fontsize=10)\n",
    "ax.legend(loc='upper left');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cfe676",
   "metadata": {},
   "source": [
    "(We have used `latex` formatting in the horizontal label, in order to format the Greek $\\lambda$ appropriately.) \n",
    "We expect the coefficient estimates to be much smaller, in terms of\n",
    "$\\ell_2$ norm, when a large value of $\\lambda$ is used, as compared to\n",
    "when a small value of $\\lambda$ is used. (Recall that the  $\\ell_2$ norm is the square root of the sum of squared coefficient values.) We display  the coefficients at the $40$th step,\n",
    "where $\\lambda$ is 25.535."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a55b76b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "beta_hat = soln_path.loc[soln_path.index[39]]\n",
    "lambdas[39], beta_hat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f4071c",
   "metadata": {},
   "source": [
    "Let’s compute the $\\ell_2$ norm of the standardized coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbee899",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.linalg.norm(beta_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2511270d",
   "metadata": {},
   "source": [
    "In contrast, here is the $\\ell_2$ norm when $\\lambda$ is 2.44e-01.\n",
    "Note the much larger $\\ell_2$ norm of the\n",
    "coefficients associated with this smaller value of $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fe1cb5",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "outputs": [],
   "source": [
    "beta_hat = soln_path.loc[soln_path.index[59]]\n",
    "lambdas[59], np.linalg.norm(beta_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c610fcf7",
   "metadata": {},
   "source": [
    "Above we normalized `X` upfront, and fit the ridge model using `Xs`.\n",
    "The `Pipeline()`  object\n",
    "in `sklearn` provides a clear way to separate feature\n",
    "normalization from the fitting of the ridge model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa46c4d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ridge = skl.ElasticNet(alpha=lambdas[59], l1_ratio=0)\n",
    "scaler = StandardScaler(with_mean=True,  with_std=True)\n",
    "pipe = Pipeline(steps=[('scaler', scaler), ('ridge', ridge)])\n",
    "pipe.fit(X, Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5607efa",
   "metadata": {},
   "source": [
    "We show that it gives the same $\\ell_2$ norm as in our previous fit on the standardized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e775d4c",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.linalg.norm(ridge.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06771f4e",
   "metadata": {
    "tags": []
   },
   "source": [
    " Notice that the operation `pipe.fit(X, Y)` above has changed the `ridge` object, and in particular has added attributes such as `coef_` that were not there before. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6a2428-a79d-49a3-9445-abb5bcb5d941",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d8b21c-6fa5-4c64-a9d4-297070dd4a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ridge.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f60acec-9d9f-4d09-91c3-c4b2758ecf76",
   "metadata": {},
   "source": [
    "### Estimating Test Error of Ridge Regression\n",
    "Choosing an *a priori* value of $\\lambda$ for ridge regression is\n",
    "difficult if not impossible. We will want to use the validation method\n",
    "or cross-validation to select the tuning parameter.\n",
    "\n",
    "We fix the random state of the splitter\n",
    "so that the results obtained will be reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da4f3f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation = skm.ShuffleSplit(n_splits=1,\n",
    "                              test_size=0.5,\n",
    "                              random_state=17092023)\n",
    "ridge.alpha = 0.01\n",
    "results = skm.cross_validate(ridge,\n",
    "                             X,\n",
    "                             Y,\n",
    "                             scoring='neg_mean_squared_error',\n",
    "                             cv=validation)\n",
    "-results['test_score']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951f3189",
   "metadata": {},
   "source": [
    "The test MSE is 1.47e+05.  Note\n",
    "that if we had instead simply fit a model with just an intercept, we\n",
    "would have predicted each test observation using the mean of the\n",
    "training observations. We can get the same result by fitting a ridge regression model\n",
    "with a *very* large value of $\\lambda$. Note that `1e10`\n",
    "means $10^{10}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6abcd6",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ridge.alpha = 1e10\n",
    "results = skm.cross_validate(ridge,\n",
    "                             X,\n",
    "                             Y,\n",
    "                             scoring='neg_mean_squared_error',\n",
    "                             cv=validation)\n",
    "-results['test_score']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60702c2d",
   "metadata": {},
   "source": [
    "Obviously choosing $\\lambda=0.01$ is arbitrary,  so we will  use cross-validation or the validation-set\n",
    "approach to choose the tuning parameter $\\lambda$.\n",
    "The object `GridSearchCV()`  allows exhaustive\n",
    "grid search to choose such a parameter.\n",
    "\n",
    "We first use the validation set method\n",
    "to choose $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4009c7c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grid = {'ridge__alpha': lambdas}\n",
    "grid = skm.GridSearchCV(pipe,\n",
    "                        param_grid,\n",
    "                        cv=validation,\n",
    "                        scoring='neg_mean_squared_error')\n",
    "grid.fit(X, Y)\n",
    "grid.best_params_['ridge__alpha']\n",
    "grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e22f625",
   "metadata": {},
   "source": [
    "Alternatively, we can use 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd51cd6",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "outputs": [],
   "source": [
    "K = 5\n",
    "kfold = skm.KFold(K,\n",
    "                  random_state=0,\n",
    "                  shuffle=True)\n",
    "\n",
    "grid = skm.GridSearchCV(pipe, \n",
    "                        param_grid,\n",
    "                        cv=kfold,\n",
    "                        scoring='neg_mean_squared_error')\n",
    "grid.fit(X, Y)\n",
    "grid.best_params_['ridge__alpha']\n",
    "grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245526c5",
   "metadata": {},
   "source": [
    "We now plot the cross-validated MSE as a function of $-\\log(\\lambda)$, which has shrinkage decreasing from left\n",
    "to right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1679fd6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ridge_fig, ax = subplots(figsize=(8,8))\n",
    "ax.errorbar(-np.log(lambdas),\n",
    "            -grid.cv_results_['mean_test_score'],\n",
    "            yerr=grid.cv_results_['std_test_score'] / np.sqrt(K))\n",
    "ax.set_ylim([50000,250000])\n",
    "ax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\n",
    "ax.set_ylabel('Cross-validated MSE', fontsize=20);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cc5ca2",
   "metadata": {},
   "source": [
    "One can cross-validate different metrics to choose a parameter. The default\n",
    "metric for `skl.ElasticNet()` is test $R^2$.\n",
    "Let’s compare $R^2$ to MSE for cross-validation here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59557ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid_r2 = skm.GridSearchCV(pipe, \n",
    "                           param_grid,\n",
    "                           cv=kfold)\n",
    "grid_r2.fit(X, Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac19760",
   "metadata": {},
   "source": [
    "Finally, let’s plot the results for cross-validated $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d94ae6",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "r2_fig, ax = subplots(figsize=(8,8))\n",
    "ax.errorbar(-np.log(lambdas),\n",
    "            grid_r2.cv_results_['mean_test_score'],\n",
    "            yerr=grid_r2.cv_results_['std_test_score'] / np.sqrt(K))\n",
    "ax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\n",
    "ax.set_ylabel('Cross-validated $R^2$', fontsize=20);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eda9aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fast Cross-Validation for Solution Paths\n",
    "The ridge, lasso, and elastic net can be efficiently fit along a sequence of $\\lambda$ values, creating what is known as a *solution path* or *regularization path*. Hence there is specialized code to fit\n",
    "such paths, and to choose a suitable value of $\\lambda$ using cross-validation. \n",
    "\n",
    "Even with identical splits the results will not agree *exactly* with our `grid`\n",
    "above because the standardization of each feature  in `grid` is carried out on each fold,\n",
    "while in `pipeCV` below it is carried out only once.\n",
    "Nevertheless, the results are similar as the normalization\n",
    "is relatively stable across folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fd1281",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ridgeCV = skl.ElasticNetCV(alphas=lambdas, \n",
    "                           l1_ratio=0,\n",
    "                           cv=kfold)\n",
    "pipeCV = Pipeline(steps=[('scaler', scaler),\n",
    "                         ('ridge', ridgeCV)])\n",
    "pipeCV.fit(X, Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8395bb",
   "metadata": {},
   "source": [
    "Let’s produce a plot again of the cross-validation error to see that\n",
    "it is similar to using `skm.GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf09512",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuned_ridge = pipeCV.named_steps['ridge'] #Calls the ridge step in pipeCV\n",
    "ridgeCV_fig, ax = subplots(figsize=(8,8))\n",
    "ax.errorbar(-np.log(lambdas),\n",
    "            tuned_ridge.mse_path_.mean(1),\n",
    "            yerr=tuned_ridge.mse_path_.std(1) / np.sqrt(K))\n",
    "ax.axvline(-np.log(tuned_ridge.alpha_), c='k', ls='--')\n",
    "ax.set_ylim([50000,250000])\n",
    "ax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\n",
    "ax.set_ylabel('Cross-validated MSE', fontsize=20);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5eb495",
   "metadata": {},
   "source": [
    "We see that the value of $\\lambda$ that results in the\n",
    "smallest cross-validation error is 1.115e-02, available\n",
    "as the value `tuned_ridge.alpha_`. What is the test MSE\n",
    "associated with this value of $\\lambda$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d6770f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.min(tuned_ridge.mse_path_.mean(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da37400",
   "metadata": {},
   "source": [
    "Note: In this line of code the function `np.min()` returns the lowest value of the MSE for all values of $\\lambda$ in our search grid.\n",
    "\n",
    "This represents a further improvement over the test MSE that we got\n",
    "using $\\lambda=0.01$.  Finally, `tuned_ridge.coef_`\n",
    "has the coefficients fit on the entire data set\n",
    "at this value of  $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852b1e39",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuned_ridge.coef_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ea5623",
   "metadata": {},
   "source": [
    "As expected, none of the coefficients are zero—ridge regression does\n",
    "not perform variable selection!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e01383a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluating Test Error of Cross-Validated Ridge\n",
    "Choosing $\\lambda$ using cross-validation provides a single regression\n",
    "estimator, similar to fitting a linear regression model. It is therefore reasonable to estimate what its test error is. \n",
    "\n",
    "We run into a problem here in that cross-validation will have\n",
    "*touched* all of its data in choosing $\\lambda$, hence we have no\n",
    "further data to estimate test error.\n",
    "\n",
    "A compromise is to do an initial\n",
    "split of the data into two disjoint sets: a training set and a test set.\n",
    "We then fit a cross-validation\n",
    "tuned ridge regression on the training set, and evaluate its performance on the test set.\n",
    "\n",
    "We might call this cross-validation nested\n",
    "within the validation set approach. A priori there is no reason to use\n",
    "half of the data for each of the two sets in validation. Below, we use\n",
    "75% for training and 25% for test, with the estimator being ridge\n",
    "regression tuned using 5-fold cross-validation.  This can be achieved\n",
    "in code as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f6259f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outer_valid = skm.ShuffleSplit(n_splits=1, \n",
    "                               test_size=0.25,\n",
    "                               random_state=1)\n",
    "inner_cv = skm.KFold(n_splits=5,\n",
    "                     shuffle=True,\n",
    "                     random_state=2)\n",
    "ridgeCV = skl.ElasticNetCV(alphas=lambdas,\n",
    "                           l1_ratio=0,\n",
    "                           cv=inner_cv)\n",
    "pipeCV = Pipeline(steps=[('scaler', scaler),\n",
    "                         ('ridge', ridgeCV)]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44495e5",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = skm.cross_validate(pipeCV, \n",
    "                             X,\n",
    "                             Y,\n",
    "                             cv=outer_valid,\n",
    "                             scoring='neg_mean_squared_error')\n",
    "-results['test_score']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fed598e",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecaca1a",
   "metadata": {},
   "source": [
    "### The Lasso\n",
    "\n",
    "Lasso coefficients minimize the following objective function: \n",
    "\n",
    "   $$ \\beta_{L}= arg min_{\\beta} \\sum_{i=1}^{n}(y_{i} - \\beta_{0} - \\sum_{j=1}^{p} \\beta_{j}x_{ij})^2 + \\lambda \\sum_{j=1}^p |\\beta_{j}| $$\n",
    "\n",
    "A consequence of this objective function is that Lasso is much more likely to shrink coefficients to exactly zero, performing variable selection. \n",
    "\n",
    "\n",
    "\n",
    "We saw that ridge regression with a wise choice of $\\lambda$ can\n",
    "outperform least squares as well as the null model on the\n",
    " `Hitters`  data set. We now ask whether the lasso can yield\n",
    "either a more accurate or a more interpretable model than ridge\n",
    "regression. In order to fit a lasso model, we once again use the\n",
    "`ElasticNetCV()`  function; however, this time we use the argument\n",
    "`l1_ratio=1`. Other than that change, we proceed just as we did in\n",
    "fitting a ridge model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae18029",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "lassoCV = skl.ElasticNetCV(n_alphas=100, \n",
    "                           l1_ratio=1,\n",
    "                           cv=kfold)\n",
    "pipeCV = Pipeline(steps=[('scaler', scaler),\n",
    "                         ('lasso', lassoCV)])\n",
    "pipeCV.fit(X, Y)\n",
    "tuned_lasso = pipeCV.named_steps['lasso']\n",
    "tuned_lasso.alpha_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d9dd06",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "outputs": [],
   "source": [
    "lambdas, soln_array = skl.Lasso.path(Xs, \n",
    "                                    Y,\n",
    "                                    l1_ratio=1,\n",
    "                                    n_alphas=100)[:2]\n",
    "soln_path = pd.DataFrame(soln_array.T,\n",
    "                         columns=D.columns,\n",
    "                         index=-np.log(lambdas))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52817006",
   "metadata": {},
   "source": [
    "We can see from the coefficient plot of the standardized coefficients that depending on the choice of\n",
    "tuning parameter, some of the coefficients will be exactly equal to\n",
    "zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd93a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "soln_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5f9a74",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_fig, ax = subplots(figsize=(8,8))\n",
    "soln_path.plot(ax=ax, legend=False)\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\n",
    "ax.set_ylabel('Standardized coefficiients', fontsize=20);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1e6626",
   "metadata": {},
   "source": [
    "The smallest cross-validated error is lower than the test set MSE of the null model\n",
    "and of least squares, and very similar to the test MSE of 115526.71 of ridge\n",
    "regression  with $\\lambda$ chosen by cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b57296a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.min(tuned_lasso.mse_path_.mean(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f35282c",
   "metadata": {},
   "source": [
    "Let’s again produce a plot of the cross-validation error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b1a84a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lassoCV_fig, ax = subplots(figsize=(8,8))\n",
    "ax.errorbar(-np.log(tuned_lasso.alphas_),\n",
    "            tuned_lasso.mse_path_.mean(1),\n",
    "            yerr=tuned_lasso.mse_path_.std(1) / np.sqrt(K))\n",
    "ax.axvline(-np.log(tuned_lasso.alpha_), c='k', ls='--')\n",
    "ax.set_ylim([50000,250000])\n",
    "ax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\n",
    "ax.set_ylabel('Cross-validated MSE', fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7250e5c0",
   "metadata": {},
   "source": [
    "However, the lasso has a substantial advantage over ridge regression\n",
    "in that the resulting coefficient estimates are **sparse**. Here we see\n",
    "that 6 of the 19 coefficient estimates are exactly zero. So the lasso\n",
    "model with $\\lambda$ chosen by cross-validation contains only 13\n",
    "variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44246aee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tuned_lasso.coef_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc08d038",
   "metadata": {},
   "source": [
    "As in ridge regression, we could evaluate the test error\n",
    "of cross-validated lasso by first splitting into\n",
    "test and training sets and internally running\n",
    "cross-validation on the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb14c121-58fa-4623-85f1-cc8090ca7f43",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed2aaf9-7a78-4240-a404-8b13c0f89777",
   "metadata": {},
   "source": [
    "In this exercise, we will predict the number of applications received using the other variables in the `College` data set:\n",
    "\n",
    "- (a) Split the data set into a training set and a test set.\n",
    "\n",
    "- (b) Fit a linear model using least squares on the training set, and report the test error obtained.\n",
    "\n",
    "- (c) Fit a ridge regression model on the training set, with `lambda` chosen by cross-validation. Report the test error obtained.\n",
    "\n",
    "- (d) Fit a lasso model on the training set, with `lambda` chosen by crossvalidation. Report the test error obtained, along with the number of non-zero coefficient estimates.\n",
    "\n",
    "- (e) Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceda3f3b-2482-460c-b7e6-6061e6114c6a",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67edd00-e895-417e-b73e-8e2c8378c462",
   "metadata": {},
   "source": [
    "In this exercise, we will generate simulated data, and will then use this data to perform lasso regressions:\n",
    "\n",
    "- (a) Create a random number generator and use its `normal()` method to generate a predictor `X` of length n = 100, as well as a noise vector $\\epsilon$ of length n = 100.\n",
    "\n",
    "- (b) Generate a response vector `Y` of length n = 100 according to the model:\n",
    "\n",
    "$$ Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3 + \\epsilon $$\n",
    "\n",
    "where $\\beta_0$, $\\beta_1$, $\\beta_2$, and $\\beta_3$ are constants of your choice.\n",
    "\n",
    "- (c) Now fit a lasso model to the simulated data, using $X$, $X^2$, ... ,$X^{10}$ as predictors. Use cross-validation to select the optimal value of $\\lambda$. Create plots of the cross-validation error as a function of $\\lambda$. Report the resulting coefficient estimates, and discuss the results obtained.\n",
    "  \n",
    "- (d) Now generate a response vector `Z` according to the model $Z =  \\beta_0 + \\beta_5X^5 + \\epsilon$,\n",
    "and perform the lasso on it with the same predictors as before. Discuss the results obtained."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,Rmd",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
