{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95a3c6ad-deaf-4897-8b4d-0a635fb92fd5",
   "metadata": {},
   "source": [
    "# Lab: Regression models\n",
    "\n",
    "This chapter follows closely chapter 3 and 4 of James et al. (2023) book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d34327-23d6-475e-a8fb-6394de61098a",
   "metadata": {},
   "source": [
    "# 1. Linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2089b886",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ISLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eb106a-8717-4f67-bd95-2eb6ae24aadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import subplots\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence \\\n",
    "     import variance_inflation_factor as VIF\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "from ISLP import load_data\n",
    "from ISLP.models import (ModelSpec as MS,\n",
    "                         summarize,\n",
    "                         poly)\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65666c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup matplotlib for graphs\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "# Set global parameters\n",
    "%matplotlib inline\n",
    "#plt.style.use('seaborn-white')\n",
    "plt.rcParams['lines.linewidth'] = 3\n",
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "plt.rcParams['figure.titlesize'] = 20\n",
    "plt.rcParams['axes.titlesize'] = 18\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['legend.fontsize'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeb05e0-7e92-4576-97f1-c00cef746216",
   "metadata": {},
   "source": [
    "Let us check if we had imported everything we need using the function `dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199e6290-a889-4129-bf4c-e89a24885a2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4140455-233a-4ea3-9865-8a6c524f9381",
   "metadata": {},
   "source": [
    "## 1. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6b3487-13bc-4281-a0f1-908d6aab4e6a",
   "metadata": {},
   "source": [
    "We will use the `Boston` housing dataset. It contains median house value (`medv`) for 506 neighborhoods in the Boston Metropolitan Area. The objective is to predict `medv` using 13 predictors (=covariates) and `statsmodels`, a package implementing reg. methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b81b947-e538-4f76-8d1e-1b49234743af",
   "metadata": {},
   "source": [
    "Load the data and check the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145f6728-2275-42be-963c-390bceb84a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Boston = load_data(\"Boston\")\n",
    "Boston.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00ec459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview of all variables\n",
    "Boston.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332433ff-d6cd-4de2-ba53-04ecd273ff91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Boston"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5052cf2",
   "metadata": {},
   "source": [
    "We can have more information on the single variables using the function describe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ca84a3-2c26-4288-b485-851f675dcd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some descriptive statistics for all the variables\n",
    "Boston.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d6b8f3",
   "metadata": {},
   "source": [
    "Let's call a single variable `medv`. We have three simple ways to do it: \n",
    "1. Use squared brackets as if the varaible was a component of a dictionary\n",
    "2. Use  dot subscripts as if the variable was a function of the data\n",
    "3. Use the `loc` function (best practice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e512d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Brackets\n",
    "Boston['medv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeac658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Dot\n",
    "Boston.medv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334af485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. The loc function\n",
    "Boston.loc[:,'medv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5914f792-f15f-4b2c-9e2b-51ef797419ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descriptive statistics for medv variable only\n",
    "Boston['medv'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367bb04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Boston['lstat'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059e6220-b685-45d9-bec9-40cfb5d1bfb0",
   "metadata": {},
   "source": [
    "Let's fit the following linear regression model. \n",
    "$$ medv = \\beta_{0} + \\beta_{1}  lstat $$\n",
    "Let's see a visual relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17cfaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fig():\n",
    "    \n",
    "    # Init figure\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    ax.set_title('Relationship between Lstat and median value');\n",
    "\n",
    "    # Plot scatter and best fit line\n",
    "    sns.regplot(x=Boston.lstat, y=Boston.medv, ax=ax, order=1, ci=None, scatter_kws={'color':'r', 's':20})\n",
    "    ax.set_xlim(-0,40); ax.set_ylim(ymin=0)\n",
    "    ax.legend(['Data','Least Square Fit']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1054564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_fig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558eb77b",
   "metadata": {},
   "source": [
    "We are creating the matrix with an intercept column (=1) and with the value of our predictor for each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8c19c9-3af2-4639-be86-f0a55abef6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame({'intercept': np.ones(Boston.shape[0]),\n",
    "                  'lstat': Boston['lstat']})\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b43acb1-ba69-4866-abea-e8b50c16abe6",
   "metadata": {},
   "source": [
    "We now extract the response associated to each row and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef4705c-29ae-407d-a924-1ce5ff641755",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Boston['medv'] #Extract the response\n",
    "model = sm.OLS(y, X) #Specifies the model \n",
    "results = model.fit() #Fit the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19463cb2-7439-4480-a606-0160d004c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize(results) #see the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512c3ed3-9d79-495d-8d8a-5898f4b8e60a",
   "metadata": {},
   "source": [
    "What can you say? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafb31b4-ac5b-453d-8076-8037ca34b0ac",
   "metadata": {},
   "source": [
    "## 1.1 Using transformations: fit and transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aa9aee-2d69-487b-b84d-586c59019fb2",
   "metadata": {},
   "source": [
    "The previous model was straitghtfoward: 1 variable, linear relationship, but in reality we are going to use much more difficult models. We can use the `sklearn` package to handle this task: _transform_. A transform is an object that is created with some parameters as arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54db614e-945c-485d-84b2-0def417e20af",
   "metadata": {},
   "source": [
    "We rely on `MS()` a general approach developed in `ISLP`. It creates a transform object, and then a pair of methods are used to construct the corresponding matrix. \n",
    "\n",
    "We first describe this process for our previous simple regression model. Here, the transform is created by the expression `design = MS(['lstat'])`.\n",
    "\n",
    "Then, `fit()`takes the original array and do computations on it (e.g., meands, sd, scaling). `transform()`applies the fitted transfromation to the array of data, and produces the model matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adfde19-95d9-485c-a696-d0f80c267fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "design = MS(['lstat']) \n",
    "design = design.fit(Boston) #Checks if the variable exists. \n",
    "X = design.transform(Boston) #Constructs the the model matrices with 2 columns\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4497df39-1d2e-44ba-baa2-57d1b1100ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "design = MS(['lstat'])\n",
    "X = design.fit_transform(Boston) #Combines fit and transform in one\n",
    "X[:4] #Shows only the first 4 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497c6aed-ac92-4d14-be55-00320c3fa088",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d00726-6c3c-42a6-a263-0d37d51a2522",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.params #Shows only the fitted coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fb369f-d08d-4953-b6b5-1e5b2fec2c62",
   "metadata": {},
   "source": [
    "Can use `get_prediction()`to obtain predictions and do inference for $\\hat{y}$ for given values of $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e346c3eb-c17f-4e88-b3b2-13207ca8c7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame({'lstat':[5, 10, 15]}) #Create a new data frame with lstat, with the values at which we want to make prediction \n",
    "newX = design.transform(new_df) #create the corresponding matrix\n",
    "newX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ebdcfe-41a3-4b23-913a-ee58b8373b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predictions = results.get_prediction(newX); #compute the prediction at newX\n",
    "new_predictions.predicted_mean #see the results by extracting the mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba527fa3-46b0-4d19-a78d-88f9a6fece9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predictions.conf_int(alpha=0.05) #produce 95% confidence intervals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35441b8-c717-44db-960b-e756a4eb1357",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predictions.conf_int(obs=True, alpha=0.05) #Prediction intervals: set obs=True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c210d98c-0a07-4013-8b67-2d919587950b",
   "metadata": {},
   "source": [
    "For instance, the 95% confidence interval associated with an `lstat` value of 5 is (29.01, 30.60), and the 95% prediction interval is (17.57, 42.04). As expected, the confidence and prediction intervals are centered around the same point (a predicted value of 29.80 for medv when lstat equals 5), but the latter are substantially wider."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d072195b-9681-43c5-8774-7dc687096036",
   "metadata": {},
   "source": [
    "### Defining functions to visualize the data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656c1ba4-7086-4922-b424-6710ccf0a536",
   "metadata": {},
   "source": [
    "Define a function called `abline` and has 3 arguments `ax, b, m`. `ax` is an axis object for an existing plot, `b`is the intercept and `m` the slope. Then, we add other plotting options into `ax.plot` using `*args` which allows any number of non-named arguments and `*kwargs` allows any number of named arguments to `abline`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4d63d4-0a77-44ce-9996-fff606f913e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abline(ax, b, m, *args, **kwargs):\n",
    "    \"Add a line with slope m and intercept b to ax\"\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = [m * xlim[0] + b, m * xlim[1] + b]\n",
    "    ax.plot(xlim, ylim, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09162a32-8748-44c2-aa97-a7161b612acc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = Boston.plot.scatter('lstat', 'medv')\n",
    "abline(ax,\n",
    "       results.params.iloc[0],\n",
    "       results.params.iloc[1],\n",
    "       'r--', # produce a red dashed line\n",
    "       linewidth=3) #make it of width 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1af824-3603-4cc8-b919-e2d907e676f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = subplots(figsize=(8,8))[1]\n",
    "ax.scatter(results.fittedvalues, results.resid)\n",
    "ax.set_xlabel('Fitted value')\n",
    "ax.set_ylabel('Residual')\n",
    "ax.axhline(0, c='k', ls='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722ccd04-1f7e-492f-aaea-56414f8bff68",
   "metadata": {},
   "source": [
    "We add a horizontal line at 0 for reference using the ax.axhline() method, indicating it should be black `(c='k')` and have a dashed linestyle `(ls='--')`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b1b983-7b16-4a30-9f9c-92e02e4addfc",
   "metadata": {},
   "source": [
    "## 1.2 Multiple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f8ee35-b524-41d4-9989-f08611223d67",
   "metadata": {},
   "source": [
    "Add an extra variable: `age`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b2f44b-ed4c-4f90-82b4-d6d86baf20cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = MS(['lstat', 'age']).fit_transform(Boston) \n",
    "model1 = sm.OLS(y, X)\n",
    "results1 = model1.fit()\n",
    "summarize(results1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1309261b-2423-453d-8230-634b6bd773f7",
   "metadata": {},
   "source": [
    "Shortcut to include all the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4897061c-8272-4d1c-b954-b421c4dd5764",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = Boston.columns.drop('medv')\n",
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f520c12-4465-4d9c-8ddd-702461b34c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = MS(terms).fit_transform(Boston)\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "summarize(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430bc1e6-5d62-4c68-a5a5-34c56e993a22",
   "metadata": {},
   "source": [
    "Can see that `age`has a high $p$-value, one might want to run a regression without it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af518498-6107-40cb-848b-9b94d69b3085",
   "metadata": {},
   "outputs": [],
   "source": [
    "minus_age = Boston.columns.drop(['medv', 'age']) \n",
    "Xma = MS(minus_age).fit_transform(Boston)\n",
    "model1 = sm.OLS(y, Xma)\n",
    "summarize(model1.fit())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386fdf80-57d6-494d-8b04-c21eda24a9c5",
   "metadata": {},
   "source": [
    "### Multivariate Goodness of Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6087ed25-71f1-4f20-a2ac-e242c7b3e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This shows us everything available\n",
    "dir(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bad1fed-6a67-42b1-ab41-25bc1242e8ef",
   "metadata": {},
   "source": [
    "Let see the $R^2$ and RSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7e87a5-2bf1-4a0b-97d5-d8df1233a1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fd96ab-6c03-4417-b933-f918b2a8f7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(results.scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6e420e-f10f-4775-9624-7a76cca2647d",
   "metadata": {},
   "source": [
    "### Interaction terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beebaca-2b78-4267-9131-e82b92ff6cca",
   "metadata": {},
   "source": [
    "The tuple `(\"lstat\",\"age\")` can be used to include an interaction term between `lstat` and ` age`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212e80c4-afbe-4974-8839-211510ac9d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = MS(['lstat',\n",
    "        'age',\n",
    "        ('lstat', 'age')]).fit_transform(Boston)\n",
    "model2 = sm.OLS(y, X)\n",
    "summarize(model2.fit())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301c0382-545a-4360-adcc-457ba3da679c",
   "metadata": {},
   "source": [
    "### Non-linear transformations of the predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0e486f-8631-443d-bda5-ba5582dab7c9",
   "metadata": {},
   "source": [
    "Can include a polynomial functions of any predictors using `poly()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eece54-2773-493d-bcce-b07040c1ed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = MS([poly('lstat', degree=2), 'age']).fit_transform(Boston)\n",
    "model3 = sm.OLS(y, X)\n",
    "results3 = model3.fit()\n",
    "summarize(results3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7df040",
   "metadata": {},
   "source": [
    "Can visualize this relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1343d5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fig_nl():\n",
    "    \n",
    "    # Figure Non linear \n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    ax.set_title('Non linear relationship')\n",
    "\n",
    "    # Plot polinomials of different degree\n",
    "    plt.scatter(x=Boston.lstat, y=Boston.medv, facecolors='None', edgecolors='k', alpha=.3) \n",
    "    sns.regplot(x=Boston.lstat, y=Boston.medv, ci=None, label='Linear', scatter=False, color='orange')\n",
    "    sns.regplot(x=Boston.lstat, y=Boston.medv, ci=None, label='Degree 2', order=2, scatter=False, color='lightblue')\n",
    "    plt.legend()\n",
    "    plt.ylim(0,40)\n",
    "    plt.xlim(0,50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b169be4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_fig_nl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa3e9a5-c683-4592-b147-425a2e6b3026",
   "metadata": {},
   "source": [
    "Can see that the the quadratic form of `lstat` is statistically significant, but does it really improve the model? We can measure the extent to which it does using `anova_lm()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb21e542-bdc3-4a14-8d88-74059f65f3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_lm(results1, results3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45608eab-6419-47b8-b46a-10e91e2bd015",
   "metadata": {},
   "source": [
    "The `anova_lm()` function performs a hypothesis test comparing the two models. The null hypothesis is that the quadratic term in the bigger model is not needed, and the alternative hypothesis is that the bigger model is superior. Here the F-statistic is 177.28 and the associated p-value is zero. In this case the F-statistic is the square of the t-statistic for the quadratic term in the linear model summary for `results3` --- a consequence of the fact that these nested models differ by one degree of freedom. This provides very clear evidence that the quadratic polynomial improves the linear model. Is it surprising? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08028294-adac-499b-844d-6764c528e7cb",
   "metadata": {},
   "source": [
    "## 1.3 Qualitative predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1970b7ee-0b24-4255-9c7d-7888f36a4807",
   "metadata": {},
   "source": [
    "Use the `Carseats`data to predict `Sales` (child car seat sales) in 400 locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf0142a-83d7-4fd7-a0e7-4b88a5ccb545",
   "metadata": {},
   "outputs": [],
   "source": [
    "Carseats = load_data('Carseats')\n",
    "Carseats.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5afbbf-dd09-4015-a7e3-9059a95b93aa",
   "metadata": {},
   "source": [
    "`ShelveLoc` is a qualitative variable. It's an indicator of the quality of the shelving location (space in the store where the seat is displayed). It takes 3 values: `Bad`, `Medium`, and `Good`. `ModelSpec()` generates dummy variables automatically, what we call _one-hot encoding_. Sum of these dummies is 1, the first column is dropped to avoid collinearity (here `Bad`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8a303f-1eec-4e60-ad03-65d2293a3a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "allvars = list(Carseats.columns.drop('Sales'))\n",
    "y = Carseats['Sales']\n",
    "final = allvars + [('Income', 'Advertising'),\n",
    "                   ('Price', 'Age')]\n",
    "X = MS(final).fit_transform(Carseats)\n",
    "model = sm.OLS(y, X)\n",
    "summarize(model.fit())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012b46ba-d66e-4766-967c-43f570bef790",
   "metadata": {},
   "source": [
    "First line: we made `allvars` a list, so that we could add the interaction terms two lines down.\n",
    "\n",
    "Our model-matrix builder has created a `ShelveLoc[Good]` dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. Same for `ShelveLoc[Medium]`.\n",
    "\n",
    "How do you interpret `ShelveLoc[Good]`? What about `ShelveLoc[Medium]`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5329e05-47f0-4034-aca5-d959f6341d80",
   "metadata": {},
   "source": [
    "# 2. Classification problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9047093e-11b0-4aac-b08f-398329f00281",
   "metadata": {},
   "source": [
    "We will use the `Smarket` data. It contains  percentage returns for the S&P 500 stock index over 1,250 days, from the beginning of 2001 until the end of 2005. For each date, we have the percentage returns for each of the five previous trading days,  `Lag1`  through\n",
    " `Lag5`. We have also recorded  `Volume`  (the number of shares traded on the previous day, in billions),  `Today`  (the percentage return on the date in question) and  `Direction` (whether the market was  `Up`  or  `Down`  on this date)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f8d66c-562d-442e-b57c-935184af7658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import subplots\n",
    "import statsmodels.api as sm\n",
    "from ISLP import load_data\n",
    "from ISLP.models import (ModelSpec as MS,\n",
    "                         summarize)\n",
    "\n",
    "#Specific to this part \n",
    "from ISLP import confusion_table\n",
    "from ISLP.models import contrast\n",
    "from sklearn.discriminant_analysis import \\\n",
    "     (LinearDiscriminantAnalysis as LDA,\n",
    "      QuadraticDiscriminantAnalysis as QDA)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a51623-0a29-483d-a301-8ffba7bee3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Smarket = load_data('Smarket')\n",
    "Smarket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7eda4e-6ed7-48fd-af5e-305f943c2bda",
   "metadata": {},
   "source": [
    "Plot the volume over time. We see that  `Volume`\n",
    "is increasing over time. In other words, the average number of shares traded\n",
    "daily increased from 2001 to 2005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba349d1-6b6b-426b-8b0f-29fa80437895",
   "metadata": {},
   "outputs": [],
   "source": [
    "Smarket.plot(y='Volume')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55536d9-1fd5-4a53-b2ad-577bee9b6471",
   "metadata": {},
   "source": [
    "### 2.1 Logistic Regression\n",
    "\n",
    "We will fit a logistic regression model to predict  `Direction`  using  `Lag1`  through  `Lag5`  and  `Volume`. The `sm.GLM()`  function fits *generalized linear models*, a class of models that includes logistic regression.  We could also have used \n",
    "the function `sm.Logit()` fits a logistic regression\n",
    "model directly. The syntax of\n",
    "`sm.GLM()` is similar to that of `sm.OLS()`, except\n",
    "that we must pass in the argument `family=sm.families.Binomial()`\n",
    "in order to tell `statsmodels` to run a logistic regression rather than some other\n",
    "type of generalized linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ba5253-b82a-4821-979e-99ec35b7c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "allvars = Smarket.columns.drop(['Today', 'Direction', 'Year']) #keep all the variables except today, direction, year\n",
    "design = MS(allvars)\n",
    "X = design.fit_transform(Smarket)\n",
    "y = Smarket.Direction == 'Up' #have to define wich value of y we set\n",
    "glm = sm.GLM(y,\n",
    "             X,\n",
    "             family=sm.families.Binomial())\n",
    "results = glm.fit()\n",
    "summarize(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c59e4a6-1c67-4ae6-98c2-a27157360055",
   "metadata": {},
   "source": [
    "What can you say? \n",
    "Few tricks to access the output: `params` to access the coefficient, `pvalues` for the $p$-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fa5124-fe53-49f4-90f2-03e8fc6bb6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c917fe-e0b1-43d6-923b-78e46b53bb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.pvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98e40f9-579b-49c2-8bd4-1aa3a78a5a1d",
   "metadata": {},
   "source": [
    "As we have seen earlier, we can use the `predict` method to predict the probability that the market will go up for specific values of the $x$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2add3d84-41af-4311-bd4d-2add3ea1234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = results.predict()\n",
    "probs[:10] #looks only at the first 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa37c57-3bbe-48a4-863d-0e7bbdc8cc96",
   "metadata": {},
   "source": [
    "The results obtained are on the **probability scale** but do not allow us to make predictions. To do so, we must convert these predicted probabilities into class labels `Up` or `Down`. \n",
    "First, we create an array with 1250 rows equal to `Down`. Then, we switch the value to `Up` if the predicted value is higher than $0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fde1a50-c9ce-40ee-9a56-9dd81364e273",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(['Down']*1250)\n",
    "labels[probs>0.5] = \"Up\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f8a44c-2d22-4a89-8b28-082070bf4d8f",
   "metadata": {},
   "source": [
    "The `confusion_table()`\n",
    "function  summarizes these predictions, showing   how\n",
    "many observations were correctly or incorrectly classified. This function, adapted from a similar function in the module `sklearn.metrics`,  transposes the resulting\n",
    "matrix and includes row and column labels.\n",
    "The `confusion_table()` function takes as first argument the\n",
    "predicted labels, and second argument the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2400a0f3-bf10-47e8-87c1-7efa123a11f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_table(labels, Smarket.Direction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28db09e-98b7-452f-a7d4-4710a054221e",
   "metadata": {},
   "source": [
    "The diagonal elements indicate correct predictions, the off-diagonal represent incorrect predictions. \n",
    "Let's use the `np.mean()` function to compute the fraction of days for which the\n",
    "prediction was correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4385f0bc-0619-410f-be27-e9582493d917",
   "metadata": {},
   "outputs": [],
   "source": [
    "(507+145)/1250, np.mean(labels == Smarket.Direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598c46d9-3a20-4a36-ae31-80df15722b05",
   "metadata": {},
   "source": [
    "What can you say? Is it a good result? What could we do next?\n",
    "Next, we examine how well it predicts the *leave out* data.  This\n",
    "will give a more realistic error rate, because we are interested in our model’s performance not on the data that\n",
    "we used to fit the model, but rather on days in the future for which\n",
    "the market’s movements are unknown.\n",
    "\n",
    "\n",
    "We first create a Boolean vector\n",
    "corresponding to the observations from 2001 through 2004 (*training data*). We  then\n",
    "use this vector to create a held out data set of observations from\n",
    "2005 (*test data*).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251602d2-5148-4b8e-840f-daae2cc7cb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = (Smarket.Year < 2005)\n",
    "Smarket_train = Smarket.loc[train]\n",
    "Smarket_test = Smarket.loc[~train]\n",
    "Smarket_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c676ba9-9e7c-4386-8bde-6c56262d951a",
   "metadata": {},
   "source": [
    "The object `train` is a vector of 1,250 elements, corresponding\n",
    "to the observations in our data set. The elements of the vector that\n",
    "correspond to observations that occurred before 2005 are set to\n",
    "`True`, whereas those that correspond to observations in 2005 are\n",
    "set to `False`.  `train` is a\n",
    "*boolean*   array, since its\n",
    "elements are `True` and `False`.  Boolean arrays can be used\n",
    "to obtain a subset of the rows or columns of a data frame\n",
    "using the `loc` method. For instance,\n",
    "the command `Smarket.loc[train]` would pick out a submatrix of the\n",
    "stock market data set, corresponding only to the dates before 2005,\n",
    "since those are the ones for which the elements of `train` are\n",
    "`True`.  The `~` symbol can be used to negate all of the\n",
    "elements of a Boolean vector. That is, `~train` is a vector\n",
    "similar to `train`, except that the elements that are `True`\n",
    "in `train` get swapped to `False` in `~train`, and vice versa.\n",
    "Therefore, `Smarket.loc[~train]` yields a\n",
    "subset of the rows of the data frame\n",
    "of the stock market data containing only the observations for which\n",
    "`train` is `False`.\n",
    "The output above indicates that there are 252 such\n",
    "observations.\n",
    "\n",
    "We now fit a logistic regression model using only the subset of the\n",
    "observations that correspond to dates before 2005. We then obtain predicted probabilities of the\n",
    "stock market going up for each of the days in our test set --- that is,\n",
    "for the days in 2005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8430eb88-dddb-4387-b0a0-c659b4a1beb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X.loc[train], X.loc[~train]\n",
    "y_train, y_test = y.loc[train], y.loc[~train]\n",
    "glm_train = sm.GLM(y_train,\n",
    "                   X_train,\n",
    "                   family=sm.families.Binomial())\n",
    "results = glm_train.fit()\n",
    "probs = results.predict(exog=X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeac5122-47e7-4d79-9146-1fe321051a4a",
   "metadata": {},
   "source": [
    "Now, we will compare the predictions for 2005 to the movement observed in our data for the same period. First, we are storing the test and training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5874f5f-a446-4ae8-905d-3840f926dccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Smarket.Direction\n",
    "L_train, L_test = D.loc[train], D.loc[~train]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c51336-5bb4-44a1-8b43-a46524eeebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(['Down']*252)\n",
    "labels[probs>0.5] = 'Up'\n",
    "confusion_table(labels, L_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305b71bd-8aa2-4937-9948-b397d38b955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(labels == L_test), np.mean(labels != L_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5cb2aa-d293-4ff1-bc30-504589de20d9",
   "metadata": {},
   "source": [
    "The test accuracy is about 48% while the error rate is about 52%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec450330-9c56-4a63-8301-705d422df8e4",
   "metadata": {},
   "source": [
    "### 2.2 K-nearest neighbors\n",
    " We fit the classifier\n",
    "using the `fit` method. New\n",
    "predictions are formed using the `predict` method\n",
    "of the object returned by `fit()`. One has to choose the number of neighbors to consider. We set $K=3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b522312f-dd10-4efb-ae12-fbfeff67573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn3 = KNeighborsClassifier(n_neighbors=3)\n",
    "X_train, X_test = [np.asarray(X) for X in [X_train, X_test]]\n",
    "knn3.fit(X_train, L_train)\n",
    "knn3_pred = knn3.predict(X_test)\n",
    "confusion_table(knn3_pred, L_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f36bf41-835c-4c2a-9dfb-87ddd7f72d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(knn3_pred == L_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7b26fe-d671-468a-ad1d-29bbc7cd6706",
   "metadata": {},
   "source": [
    "It performs slightly better than the logisitic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f2ac26-0846-4a6d-a72b-eb4cb4fc179c",
   "metadata": {},
   "source": [
    "KNN is a powerful classifier, let us go further with another dataset. To show it, we will use the `Caravan`  data set. This data set includes 85\n",
    "predictors that measure demographic characteristics for 5,822\n",
    "individuals. The response variable is  `Purchase`, which\n",
    "indicates whether or not a given individual purchases a caravan\n",
    "insurance policy. In this data set, only 6% of people purchased\n",
    "caravan insurance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7c2023-a4d3-404b-9d16-ac48097d6765",
   "metadata": {},
   "outputs": [],
   "source": [
    "Caravan = load_data('Caravan')\n",
    "Purchase = Caravan.Purchase\n",
    "Purchase.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1b618f-ad59-4bd6-b61c-bcb4cdaab372",
   "metadata": {},
   "source": [
    "We create a feature dataframe including all columns except `Purchase`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b84d94b-12e2-460c-afe6-f9409d0286a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = Caravan.drop(columns=['Purchase'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d408205f-0c3c-4d68-940b-4b685914f7f6",
   "metadata": {},
   "source": [
    "KNN  predicts the class of a given test\n",
    "observation by identifying the observations that are nearest to it, hence\n",
    "the scale of the variables matters. Any variables that are on a large\n",
    "scale will have a much larger effect on the *distance* between\n",
    "the observations, and hence on the KNN accuracy, than variables that\n",
    "are on a small scale. Furthermore, the\n",
    "importance of scale to the KNN classifier leads to another issue: if\n",
    "we measured  `salary`  in Japanese yen, or if we measured\n",
    " `age`  in minutes, then we’d get quite different classification\n",
    "results from what we get if these two variables are measured in\n",
    "dollars and years.\n",
    "\n",
    "How to solve such problem? A good way to deal with it is to *standardize*  the data to have all of them on a comparable scale. This is accomplished\n",
    "using the `StandardScaler()`\n",
    "transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c391ff50-f49a-4f50-afb5-e1223e797233",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(with_mean=True, #to substract the mean\n",
    "                        with_std=True, #scale the column with sd=1\n",
    "                        copy=True) #copy the data and not erase it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478be87f-9ec6-4fcd-b228-49eca3a5309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(feature_df) #compute the parameters for the scaling and stored in scaler\n",
    "X_std = scaler.transform(feature_df) #construct the standardize x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ed5c2c-6511-4cdd-a0ad-a9518df27375",
   "metadata": {},
   "source": [
    "Check if it had worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d82fca-e475-46e6-9e1c-e9d5b8ab3253",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_std = pd.DataFrame(\n",
    "                 X_std,\n",
    "                 columns=feature_df.columns);\n",
    "feature_std.std()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437bc8c5-7efc-4c01-a03c-1dc6fc47df4d",
   "metadata": {},
   "source": [
    "Using the function `train_test_split()`  we now split the observations into a test set,\n",
    "containing 1000 observations, and a training set containing the remaining\n",
    "observations. The argument `random_state=0` ensures that we get\n",
    "the same split each time we rerun the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09ae274-604a-433c-843a-07bf7fb203d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,\n",
    " X_test,\n",
    " y_train,\n",
    " y_test) = train_test_split(np.asarray(feature_std),\n",
    "                            Purchase,\n",
    "                            test_size=1000,\n",
    "                            random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc0625c-078d-43b8-a386-edd699b5371b",
   "metadata": {},
   "source": [
    "We now fit a KNN model on the training data using $K=1$ and evaluate its performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a866778-8cd6-4c7a-8fcc-871896aab0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn1 = KNeighborsClassifier(n_neighbors=1)\n",
    "knn1_pred = knn1.fit(X_train, y_train).predict(X_test)\n",
    "np.mean(y_test != knn1_pred), np.mean(y_test != \"No\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f909d7-f3bf-46a0-9d39-fec88fb9c4af",
   "metadata": {},
   "source": [
    "The KNN error rate is about 11%, is it a good result? Is the overall error rate the most relevant prediction performance indicator for an insurance company with a non-trivial cost of selling insurance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8821ad1e-d022-4a7c-b3ec-5b02f33e8570",
   "metadata": {},
   "source": [
    "#### Tuning parameters\n",
    "\n",
    "The number of neighbors in KNN is referred to as a *tuning parameter* (or hyperparameter*).\n",
    "We do not know *a priori* what value to use. It is therefore of interest\n",
    "to see how the classifier performs on test data as we vary these\n",
    "parameters. This can be achieved with a `for` loop to construct a *search grid*.\n",
    "Here we use a for loop to look at the accuracy of our classifier in the group predicted to purchase\n",
    "insurance as we vary the number of neighbors from 1 to 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61282d8-0908-448f-9e4c-286cd46fb9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for K in range(1,6):\n",
    "    knn = KNeighborsClassifier(n_neighbors=K)\n",
    "    knn_pred = knn.fit(X_train, y_train).predict(X_test)\n",
    "    C = confusion_table(knn_pred, y_test)\n",
    "    templ = ('K={0:d}: # predicted to rent: {1:>2},' +\n",
    "            '  # who did rent {2:d}, accuracy {3:.1%}')\n",
    "    pred = C.loc['Yes'].sum()\n",
    "    did_rent = C.loc['Yes','Yes']\n",
    "    print(templ.format(\n",
    "          K,\n",
    "          pred,\n",
    "          did_rent,\n",
    "          did_rent / pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea3fcce-7211-48ff-ba03-35703cbc6161",
   "metadata": {},
   "source": [
    "Does it perform better than a logit? We use `sklearn` but it fits something like the *ridge regression* version of logistic regression. We modified it by setting the argument `C` to a very large number, hence it converges to the same solution as a logit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68c0824-49d6-463b-9b8b-06a7e7012d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(C=1e10, solver='liblinear') #liblinear allows convergence \n",
    "logit.fit(X_train, y_train)\n",
    "logit_pred = logit.predict_proba(X_test)\n",
    "logit_labels = np.where(logit_pred[:,1] > .5, 'Yes', 'No')\n",
    "confusion_table(logit_labels, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0cef36-2e5a-4e65-9647-8115b290f573",
   "metadata": {},
   "source": [
    "If we use $0.5$ as the predicted probability cut-off for the\n",
    "classifier, then we have a problem: only two of the test observations\n",
    "are predicted to purchase insurance.  However, we are not required to use a\n",
    "cut-off of $0.5$. If we instead predict a purchase any time the\n",
    "predicted probability of purchase exceeds $0.25$, we get much better\n",
    "results: we predict that 29 people will purchase insurance, and we are\n",
    "correct for about 31% of these people. This is almost five times\n",
    "better than random guessing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b03a9cf-fa54-4e96-9455-921c28d0c6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_labels = np.where(logit_pred[:,1]>0.25, 'Yes', 'No')\n",
    "confusion_table(logit_labels, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37b2a3a-8951-4b9c-b0f2-e57fd362f517",
   "metadata": {},
   "outputs": [],
   "source": [
    "9/(20+9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9dbc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f163c44-7895-4342-ae6e-5617463e6ef4",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "From James et al. (2023), pp. 196-197.\n",
    "\n",
    "This question should be answered using the Weekly data set, which\n",
    "is part of the ISLP package. This data is similar in nature to the\n",
    "Smarket data from this chapter’s lab, except that it contains 1, 089\n",
    "weekly returns for 21 years, from the beginning of 1990 to the end of\n",
    "2010.\n",
    "\n",
    "(a) Produce some numerical and graphical summaries of the Weekly\n",
    "data. Do there appear to be any patterns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d8eb64-d268-42df-8813-33cd4d1e7bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d035641-1147-4e2f-a8b2-f508f89793bf",
   "metadata": {},
   "source": [
    "(b) Use the full data set to perform a logistic regression with\n",
    "Direction as the response and the five lag variables plus Volume\n",
    "as predictors. Use the summary function to print the results. Do\n",
    "any of the predictors appear to be statistically significant? If so,\n",
    "which ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4411d00f-aa35-4433-a734-c83c01c786d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a86f5af-a25d-4202-9861-bdf72f997b21",
   "metadata": {},
   "source": [
    "(c) Compute the confusion matrix and overall fraction of correct\n",
    "predictions. Explain what the confusion matrix is telling you\n",
    "about the types of mistakes made by logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5f2a4e-f711-4906-ac66-321734b7452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d04c0d-512a-4d8c-9065-6989ab0f4c69",
   "metadata": {},
   "source": [
    "(d) Now fit the logistic regression model using a training data period\n",
    "from 1990 to 2008, with Lag2 as the only predictor. Compute the\n",
    "confusion matrix and the overall fraction of correct predictions\n",
    "for the held out data (that is, the data from 2009 and 2010)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8c50d4-6c25-40ef-963f-a4e87c484071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de68ede1-280e-435c-94ff-c37d3cfd5ee2",
   "metadata": {},
   "source": [
    "(g) Repeat (d) using KNN with K = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe458a4e-79b4-4317-bc18-da5cf7715eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfda06d-6212-4675-b5f4-733a655bd1d9",
   "metadata": {},
   "source": [
    "(i) Which of these methods appears to provide the best results on\n",
    "this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad79813f-bae3-4fa2-9c45-40a9d6d00d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Type your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
